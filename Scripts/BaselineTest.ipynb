{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9eefc47-bbb1-4b41-9e7f-5ef731463776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a37358dfb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Putting all the imports in one place for readability\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "from collections import Counter\n",
    "import Anotation\n",
    "import copy\n",
    "from allennlp.modules.conditional_random_field import ConditionalRandomField as CRF\n",
    "from torcheval.metrics.functional import multiclass_confusion_matrix as MCM\n",
    "\n",
    "# Setting seeds to ensure reproducibility of results\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a67f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts the data into 2 lists of lists, one with the tokens another with the tags\n",
    "\n",
    "\n",
    "def extractData(filePath):\n",
    "    \"\"\"\n",
    "    Returns:tuple: A tuple containing input data (list of lists of words), tags (list of lists of tags),\n",
    "    and metadata (list of tuples containing newdoc_id, sent_id, and text).\n",
    "    \"\"\"\n",
    "    wordsData = []\n",
    "    tagsData = []\n",
    "    currentSent = None\n",
    "    with open(filePath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# sent_id\"):\n",
    "                sentId = line.split(\"= \")[1]\n",
    "            elif line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line:                \n",
    "                parts = line.split('\\t')\n",
    "                word = parts[1]\n",
    "                tag = parts[2]\n",
    "                if sentId != currentSent:\n",
    "                    currentSent = sentId\n",
    "                    wordsData.append([word])\n",
    "                    tagsData.append([tag])\n",
    "                else:\n",
    "                    wordsData[-1].append(word)\n",
    "                    tagsData[-1].append(tag)\n",
    "    return wordsData, tagsData\n",
    "\n",
    "# Example usage:\n",
    "#file_path = \"../Data/UniversalNER/train/en_ewt-ud-train.iob2\"\n",
    "#words_data, tags_data = extractData(file_path)\n",
    "# for words, tags in zip(words_data, tags_data):\n",
    "#     print(\"Words:\", words)\n",
    "#     print(\"Tags:\", tags)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380bc832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  7,  8, 10, 11,  9,  6,  4,  2],\n",
      "        [ 1,  6,  5,  2,  0,  0,  0,  0,  0]])\n",
      "tensor([[1, 5, 5, 5, 5, 5, 3, 5, 2],\n",
      "        [1, 3, 4, 2, 0, 0, 0, 0, 0]])\n",
      "{'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3, '?': 4, 'Falls': 5, 'Iguazu': 6, 'Where': 7, 'in': 8, 'is': 9, 'the': 10, 'world': 11}\n",
      "{'<PAD>': 0, '<START>': 1, '<END>': 2, 'B-LOC': 3, 'I-LOC': 4, 'O': 5}\n"
     ]
    }
   ],
   "source": [
    "#Converts the Data into a tensor for use by the model\n",
    "\n",
    "def convertDataShape(data, vocabulary = {}, labels = [], training = True, PADDING_TOKEN = '<PAD>', START_TOKEN = '<START>', STOP_TOKEN = '<END>', UNKNOWN_TOKEN = '<UNK>'):\n",
    "    \"\"\"\n",
    "    If training is enabled creates a vocabulary of all words in a list. Otherwise, a vocabulary should be passed.\n",
    "    Does the same with the labels.\n",
    "    Creates a matrix of sentences and positions, where each value indicates a word via its index in the vocabulary.\n",
    "    Creates another matrix of sentences and positions, where the values indicate a label.\n",
    "    '<PAD>' or another user defined token is used as padding for short sentences. Will also act as an unknown token, if not training, it is assumed to be in vocabulary.\n",
    "    Returns, the vocabulary, the labels and the two matrices.\n",
    "    \n",
    "    Input:\n",
    "    data          - (string list * string list) list - List of sentences. Each sentence is a tuple of two lists. The first is a list of words, the second a list of labels.\n",
    "    vocabulary    - string : int dictionary          - Dictionary of words in the vocabulary, values are the indices. Should be provided if not training. Defaults to empty dict.\n",
    "    labels        - string : int dictionary          - Dictionary of labels to classify, values are the indices. Should be provided if not training. Defaults to empty dict.\n",
    "    training      - boolean                          - Boolean variable deffining whether training is taking place, if yes then a new vocabulary will be created. Defaults to yes.\n",
    "    PADDING_TOKEN - string                           - Token to be used as padding. Default is provided\n",
    "    START_TOKEN   - string                           - Token to be used as marker for the start of the sentence. Default is provided\n",
    "    STOP_TOKEN    - string                           - Token to be used as marker for the end of the sentence. Default is provided\n",
    "    UNKNOWN_TOKEN - string                           - Token to be used as the unknown token. Default is provided\n",
    "    \n",
    "    Output:\n",
    "    Xmatrix       - 2D torch.tensor                  - 2d torch tensor containing the index of the word in the sentence in the vocabulary\n",
    "    Ymatrix       - 2D torch.tensor                  - 2d torch tensor containing the index of the label in the sentence in the labels\n",
    "    vocabulary    - string : int dictionary          - Dictionary of words, with indices as values, used for training.\n",
    "    labels        - string : int dictionary          - Dictionary of all the labels, with indices as values, used for classification. (all the labels are expected to be present in the training data, or in other words, the label list provided should be exhaustive)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if training:\n",
    "        vocabList = sorted(set(word for sentence, _ in data for word in sentence))\n",
    "        \n",
    "        #In order to be able to work with unknown words in the future, we turn some of the least common words into unknown words so we can train on them\n",
    "        #This is done by removing them from the vocab list before creating the dictionary\n",
    "        vocabCount = Counter([word for sentence, _ in data for word in sentence])\n",
    "        UNKNOWN_RATIO = 5 #This should be percentage of tokens we want to turn into Unknown tokens, the least common tokens will be used\n",
    "        cutoff = int(len(vocabList) / (100 / UNKNOWN_RATIO)) + 1\n",
    "        removeList = vocabCount.most_common()[:-cutoff:-1]\n",
    "        for i in removeList:\n",
    "            vocabList.remove(i[0])\n",
    "\n",
    "        # Adding the special tokens in the first positions after the least common have been removed and creating the dictionaries\n",
    "        vocabList = [PADDING_TOKEN, START_TOKEN, STOP_TOKEN, UNKNOWN_TOKEN] + vocabList\n",
    "        vocabulary = {word: i for i, word in enumerate(vocabList)}\n",
    "        labelList = [PADDING_TOKEN, START_TOKEN, STOP_TOKEN] + sorted(set(label for _, sentenceLabels in data for label in sentenceLabels))\n",
    "        labels = {label: i for i, label in enumerate(labelList)}\n",
    "    \n",
    "    # Adding two to the max len in order to accomodate the introduction of start and end tokens\n",
    "    maxLen = max(len(sentence) for sentence, _ in data) + 2\n",
    "    Xmatrix = np.zeros((len(data), maxLen), dtype=int)\n",
    "    Ymatrix = np.zeros((len(data), maxLen), dtype=int)\n",
    "\n",
    "    for i, (sentence, sentenceLabels) in enumerate(data):\n",
    "        #Set the first token as the start token (assumes it's index is 1)\n",
    "        Xmatrix[i, 0] = 1\n",
    "        Ymatrix[i, 0] = 1\n",
    "        #Set all the indices to the correct index, with the unknown token as default\n",
    "        for j, word in enumerate(sentence):\n",
    "            Xmatrix[i, j+1] = vocabulary.get(word, vocabulary[UNKNOWN_TOKEN])\n",
    "        for j, label in enumerate(sentenceLabels):\n",
    "            Ymatrix[i, j+1] = labels.get(label, labels[START_TOKEN])\n",
    "            lastWord = j         \n",
    "        # Sets the token after the last word as en end token\n",
    "        Xmatrix[i, lastWord + 2] = 2\n",
    "        Ymatrix[i, lastWord + 2] = 2\n",
    "    \n",
    "    return torch.tensor(Xmatrix, dtype=torch.long), torch.tensor(Ymatrix, dtype=torch.long), vocabulary, labels\n",
    "\n",
    "# two first sentences of EWT training dataset so that quickdebugging can be run\n",
    "\n",
    "\n",
    "\n",
    "trainingDebugSen = [[\"Where\", \"in\", \"the\", \"world\", \"is\", \"Iguazu\", \"?\"], [\"Iguazu\", \"Falls\"]]\n",
    "trainingDebugTags = [[\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"], [\"B-LOC\", \"I-LOC\"]]\n",
    "\n",
    "dataDebug, labelsDebug, vocabDebug, tagsDebug = convertDataShape(list(zip(trainingDebugSen, trainingDebugTags)))\n",
    "print(dataDebug)\n",
    "print(labelsDebug)\n",
    "print(vocabDebug)\n",
    "print(tagsDebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb63731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baselineModel(torch.nn.Module):\n",
    "    def __init__(self, nWords, tags, dimEmbed, dimHidden, constraints, dropOut):\n",
    "        super().__init__()\n",
    "        self.dimEmbed = dimEmbed\n",
    "        self.dimHidden = dimHidden\n",
    "        self.vocabSize = nWords\n",
    "        self.tagSetSize = len(tags)\n",
    "\n",
    "        self.embed = nn.Embedding(nWords, dimEmbed)\n",
    "        self.LSTM = nn.LSTM(dimEmbed, dimHidden, bidirectional=True, dropout = dropOut)\n",
    "        self.linear = nn.Linear(dimHidden * 2, self.tagSetSize)\n",
    "        \n",
    "\n",
    "        # Initialize the CRF layer\n",
    "        self.CRF = CRF(self.tagSetSize, constraints = constraints, include_start_end_transitions=True)\n",
    "\n",
    "    def forwardTrain(self, inputData, labels):\n",
    "        # Embedding and LSTM layers\n",
    "        wordVectors = self.embed(inputData)\n",
    "        lstmOut, _ = self.LSTM(wordVectors)\n",
    "        \n",
    "        # Linear layer\n",
    "        emissions = self.linear(lstmOut)\n",
    "        \n",
    "        # CRF layer to compute the log likelihood loss\n",
    "        log_likelihood = self.CRF(emissions, labels)\n",
    "        \n",
    "        # The loss is the negative log-likelihood\n",
    "        loss = -log_likelihood\n",
    "        return loss\n",
    "        \n",
    "    def forwardPred(self, inputData):\n",
    "        # Embedding and LSTM layers\n",
    "        wordVectors = self.embed(inputData)\n",
    "        lstmOut, _ = self.LSTM(wordVectors)\n",
    "        \n",
    "        # Linear layer\n",
    "        emissions = self.linear(lstmOut)\n",
    "        \n",
    "        # Decode the best path\n",
    "        best_paths = self.CRF.viterbi_tags(emissions)\n",
    "        \n",
    "        # Extract the predicted tags from the paths\n",
    "        predictions = [path for path, score in best_paths]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c82f7-6c93-469b-9913-7483ed57ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saveToIob2(words, labels, outputFilePath):\n",
    "    \"\"\"\n",
    "    Save words and their corresponding labels in IOB2 format.\n",
    "\n",
    "    Args:\n",
    "    words (list): List of lists containing words.\n",
    "    labels (list): List of lists containing labels.\n",
    "    output_file (str): Path to the output IOB2 file.\n",
    "    \"\"\"\n",
    "    with open(outputFilePath, 'w', encoding='utf-8') as file:\n",
    "        for i in range(len(words)):\n",
    "            for j in range(len(words[i])):\n",
    "                line = f\"{j+1}\\t{words[i][j]}\\t{labels[i][j]}\\n\"\n",
    "                file.write(line)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d166f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small dataset\n",
    "# two first sentences of EWT training dataset so that quickdebugging can be run\n",
    "\n",
    "tags = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\"]\n",
    "\n",
    "trainingDebugSen = [[\"Where\", \"in\", \"the\", \"world\", \"is\", \"Iguazu\", \"?\"], [\"Iguazu\", \"Falls\"]]\n",
    "trainingDebugTags = [[\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"], [\"B-LOC\", \"I-LOC\"]]\n",
    "\n",
    "dataDebug, labelsDebug, vocabDebug, tagsDebug = convertDataShape(list(zip(trainingDebugSen, trainingDebugTags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test train\n",
    "#Quick traininig script on the debug dataset\n",
    "\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)\n",
    "\n",
    "constraint_type = None\n",
    "\n",
    "model = baselineModel(len(vocabDebug), tagsDebug, DIM_EMBEDDING, LSTM_HIDDEN, constraint_type, 0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model.forwardTrain(dataDebug, labelsDebug)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68fee95-753b-4a42-b2ce-cae881316205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predicitons and checking accuracy\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictsDebug = model.forwardPred(dataDebug)\n",
    "\n",
    "confMat = MCM(torch.flatten(torch.tensor(predictsDebug, dtype=torch.long)), torch.flatten(labelsDebug), num_classes = len(tagsDebug))\n",
    "\n",
    "acc = torch.trace(confMat[1:,1:])/torch.sum(confMat[1:,1:]) #Taking away the first collumn and first row, because those correspond to the padding token and we don't care\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e1c711c-55e7-48e1-8bd9-b6d1c6ed3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the training data sets\n",
    "#pass train data size\n",
    "def loadingAllData(train = 0.8):\n",
    "    filePathTrain = \"../Data/UniversalNER/train/\"   \n",
    "    wordsData = []\n",
    "    tagsData = []\n",
    "    datasets = [\"da_ddt\", \"en_ewt\", \"hr_set\", \"pt_bosque\", \"sk_snk\", \"sr_set\", \"sv_talbanken\", \"zh_gsdsimp\", \"zh_gsd\"]\n",
    "\n",
    "    for i in datasets:\n",
    "        wordsDataTemp, tagsDataTemp = extractData(filePathTrain + i + \"-ud-train.iob2\")\n",
    "        wordsData += wordsDataTemp\n",
    "        tagsData += tagsDataTemp\n",
    "\n",
    "    wordsDataAnot, tagsDataAnot = Anotation.read_file_and_split(\"../Data/LotR/LabelingFinals/AllSentences.txt\")\n",
    "    wordsData += wordsDataAnot[0:int(len(wordsDataAnot)*train)]\n",
    "    tagsData += tagsDataAnot[0:int(len(wordsDataAnot)*train)]\n",
    "    testWordsData = wordsDataAnot[int(len(wordsDataAnot)*train):]\n",
    "    testTagsData = tagsDataAnot[int(len(wordsDataAnot)*train):]\n",
    "    trainData, trainLabels, vocab, labels = convertDataShape(list(zip(wordsData, tagsData)))\n",
    "    testData, testLabels, _, _ = convertDataShape(list(zip(testWordsData, testTagsData)), training = False, vocabulary = vocab, labels = labels)\n",
    "    return trainData, trainLabels, vocab, labels, testData, testLabels, testWordsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdbde3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsDataTemp, tagsDataTemp = Anotation.read_file_and_split(\"../Data/LotR/LabelingFinals/AllSentences.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef3f8b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1602\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsDataTemp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7508cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataAll, trainLabelsAll, vocab, labels, testData, testLabels, testWords = loadingAllData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fa51bee-3da8-44ca-84ab-fb4086c2c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataEWT = trainDataAll[:-1281]  # 1281 is the floor of 1602 * 0.8, could be substituted by the expression but unsure if we will change it\n",
    "trainDataLotR = trainDataAll[-1281:]\n",
    "trainLabelsEWT = trainLabelsAll[:-1281]\n",
    "trainLabelsLotR = trainLabelsAll[-1281:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "619a0761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1281, 298])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataLotR.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add8989-9183-426d-97c3-b0b3f5e66093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 491.60546875\n",
      "Epoch 1, Loss: 239.083984375\n",
      "Epoch 2, Loss: 157.78466796875\n",
      "Epoch 3, Loss: 105.6962890625\n",
      "Epoch 4, Loss: 65.884033203125\n",
      "Epoch 5, Loss: 43.292724609375\n",
      "Epoch 6, Loss: 29.643798828125\n",
      "Epoch 7, Loss: 21.016357421875\n",
      "Epoch 8, Loss: 16.256103515625\n",
      "Epoch 9, Loss: 15.9052734375\n",
      "Epoch 10, Loss: 20.338134765625\n",
      "Epoch 11, Loss: 23.85546875\n",
      "Epoch 12, Loss: 33.167236328125\n",
      "Epoch 13, Loss: 24.781982421875\n",
      "Epoch 14, Loss: 18.210205078125\n",
      "Epoch 15, Loss: 22.462646484375\n",
      "Epoch 16, Loss: 11.548828125\n",
      "Epoch 17, Loss: 9.3505859375\n",
      "Epoch 18, Loss: 8.751953125\n",
      "Epoch 19, Loss: 14.257080078125\n"
     ]
    }
   ],
   "source": [
    "DIM_EMBEDDING = 300\n",
    "LSTM_HIDDEN = 100\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "\n",
    "\n",
    "PADDING_TOKEN = '<PAD>'\n",
    "START_TOKEN = '<START>'\n",
    "STOP_TOKEN = '<END>'\n",
    "# The make constraint from the module was yielding some weird results so I decided to hardcode this for our use case, assuming the following dict of tags\n",
    "#{'<PAD>': 0, '<START>': 1, '<END>': 2, '-': 3, 'B-LOC': 4, 'B-ORG': 5, 'B-OTH': 6, 'B-PER': 7, 'I-LOC': 8, 'I-ORG': 9, 'I-OTH': 10, 'I-PER': 11, 'O': 12}\n",
    "CONSTRAINTS = [(0, 0), (1, 4), (1, 5), (1, 6), (1, 7), (1, 10), (1, 12), (2, 0), (4, 2), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 12), \n",
    "              (5, 2), (5, 4), (5, 5), (5, 6), (5, 7), (5, 9), (5, 12), (6, 2), (6, 4), (6, 5), (6, 6), (6, 7), (6, 10), (6, 12),\n",
    "              (7, 2), (7, 4), (7, 5), (7, 6), (7, 7), (7, 11), (7, 12), (8, 2), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 12),\n",
    "              (9, 2), (9, 4), (9, 5), (9, 6), (9, 7), (9, 9), (9, 12), (10, 2), (10, 4), (10, 5), (10, 6), (10, 7), (10, 10), (10, 12),\n",
    "              (11, 2), (11, 4), (11, 5), (11, 6), (11, 7), (11, 11), (11, 12), (12, 2), (12, 4), (12, 5), (12, 6), (12, 7), (12, 12)]\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)\n",
    "\n",
    "numBatches = trainDataEWT.shape[0] // BATCH_SIZE\n",
    "\n",
    "trainDataBatches = trainDataEWT[:BATCH_SIZE*numBatches].view(numBatches, BATCH_SIZE, trainDataEWT.shape[1])\n",
    "trainLabelsBatches = trainLabelsEWT[:BATCH_SIZE*numBatches].view(numBatches, BATCH_SIZE,  trainLabelsEWT.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "model = baselineModel(len(vocab), labels, DIM_EMBEDDING, LSTM_HIDDEN, CONSTRAINTS, DROPOUT)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    for batch in zip(trainDataBatches, trainLabelsBatches): \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model.forwardTrain(batch[0], batch[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "     \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b8179fd-ecbe-4dc2-ab8a-1e185969e4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0, Epoch: 0, Loss: 158.518310546875\n",
      "Model: 0, Epoch: 1, Loss: 158.518310546875\n",
      "Model: 0, Epoch: 2, Loss: 158.518310546875\n",
      "Model: 0, Epoch: 3, Loss: 158.518310546875\n",
      "Model: 0, Epoch: 4, Loss: 158.518310546875\n",
      "Model: 1, Epoch: 0, Loss: 143.449951171875\n",
      "Model: 1, Epoch: 1, Loss: 143.449951171875\n",
      "Model: 1, Epoch: 2, Loss: 143.449951171875\n",
      "Model: 1, Epoch: 3, Loss: 143.449951171875\n",
      "Model: 1, Epoch: 4, Loss: 143.449951171875\n",
      "Model: 2, Epoch: 0, Loss: 114.98046875\n",
      "Model: 2, Epoch: 1, Loss: 114.98046875\n",
      "Model: 2, Epoch: 2, Loss: 114.98046875\n",
      "Model: 2, Epoch: 3, Loss: 114.98046875\n",
      "Model: 2, Epoch: 4, Loss: 114.98046875\n",
      "Model: 3, Epoch: 0, Loss: 113.05224609375\n",
      "Model: 3, Epoch: 1, Loss: 113.05224609375\n",
      "Model: 3, Epoch: 2, Loss: 113.05224609375\n",
      "Model: 3, Epoch: 3, Loss: 113.05224609375\n",
      "Model: 3, Epoch: 4, Loss: 113.05224609375\n",
      "Model: 4, Epoch: 0, Loss: 120.353271484375\n",
      "Model: 4, Epoch: 1, Loss: 120.353271484375\n",
      "Model: 4, Epoch: 2, Loss: 120.353271484375\n",
      "Model: 4, Epoch: 3, Loss: 120.353271484375\n",
      "Model: 4, Epoch: 4, Loss: 120.353271484375\n",
      "Model: 5, Epoch: 0, Loss: 142.541748046875\n",
      "Model: 5, Epoch: 1, Loss: 142.541748046875\n",
      "Model: 5, Epoch: 2, Loss: 142.541748046875\n",
      "Model: 5, Epoch: 3, Loss: 142.541748046875\n",
      "Model: 5, Epoch: 4, Loss: 142.541748046875\n",
      "Model: 6, Epoch: 0, Loss: 93.576416015625\n",
      "Model: 6, Epoch: 1, Loss: 93.576416015625\n",
      "Model: 6, Epoch: 2, Loss: 93.576416015625\n",
      "Model: 6, Epoch: 3, Loss: 93.576416015625\n",
      "Model: 6, Epoch: 4, Loss: 93.576416015625\n",
      "Model: 7, Epoch: 0, Loss: 201.400390625\n",
      "Model: 7, Epoch: 1, Loss: 201.400390625\n",
      "Model: 7, Epoch: 2, Loss: 201.400390625\n",
      "Model: 7, Epoch: 3, Loss: 201.400390625\n",
      "Model: 7, Epoch: 4, Loss: 201.400390625\n",
      "Model: 8, Epoch: 0, Loss: 177.28173828125\n",
      "Model: 8, Epoch: 1, Loss: 177.28173828125\n",
      "Model: 8, Epoch: 2, Loss: 177.28173828125\n",
      "Model: 8, Epoch: 3, Loss: 177.28173828125\n",
      "Model: 8, Epoch: 4, Loss: 177.28173828125\n",
      "Model: 9, Epoch: 0, Loss: 199.093994140625\n",
      "Model: 9, Epoch: 1, Loss: 199.093994140625\n",
      "Model: 9, Epoch: 2, Loss: 199.093994140625\n",
      "Model: 9, Epoch: 3, Loss: 199.093994140625\n",
      "Model: 9, Epoch: 4, Loss: 199.093994140625\n"
     ]
    }
   ],
   "source": [
    "#fine tunning all of the different models\n",
    "NUMBER_OF_MODELS_WITH_FINETUNNING = 10 # 10 models with some degree of fine tunning, one without, jumping in increments of 10 % of data\n",
    "FINETUNNING_LEARNING_RATE = 0.001\n",
    "FINETUNNING_EPOCHS = 5\n",
    "FINETUNNING_BATCH_SIZE = 32 # given this batch size and the size of our dataset, there are 4 batches in every 10% of data\n",
    "\n",
    "finetunningNumBatches = trainDataLotR.shape[0] // FINETUNNING_BATCH_SIZE\n",
    "\n",
    "# Reshape data into batches\n",
    "trainDataBatchesLotR = trainDataLotR[:FINETUNNING_BATCH_SIZE * finetunningNumBatches].view(finetunningNumBatches, FINETUNNING_BATCH_SIZE, trainDataLotR.shape[1])\n",
    "trainLabelsBatchesLotR = trainLabelsLotR[:FINETUNNING_BATCH_SIZE * finetunningNumBatches].view(finetunningNumBatches, FINETUNNING_BATCH_SIZE, trainLabelsLotR.shape[1])\n",
    "\n",
    "modelBase = model\n",
    "finetunnedModels = [copy.deepcopy(model) for i in range(NUMBER_OF_MODELS_WITH_FINETUNNING)]\n",
    "\n",
    "for i, modelToTrain in enumerate(finetunnedModels):\n",
    "\n",
    "    finetunningOptimizer = torch.optim.Adam(model.parameters(), lr = FINETUNNING_LEARNING_RATE)\n",
    "    \n",
    "    for epoch in range(FINETUNNING_EPOCHS):\n",
    "        modelToTrain.train()\n",
    "\n",
    "        modelToTrain.zero_grad()\n",
    "\n",
    "        j = 0\n",
    "        for batch in zip(trainDataBatchesLotR, trainLabelsBatchesLotR):\n",
    "\n",
    "            if j == (i+1)*4: #Making sure that each model only accesses the batches it should\n",
    "                break\n",
    "            \n",
    "            finetunningOptimizer.zero_grad()\n",
    "            loss = modelToTrain.forwardTrain(batch[0], batch[1])\n",
    "            loss.backward()\n",
    "            finetunningOptimizer.step()\n",
    "            j += 1\n",
    "     \n",
    "    \n",
    "        print(f\"Model: {i}, Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc8ffe0c-93c8-4577-9ad1-c61f7554f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the results\n",
    "\n",
    "\n",
    "for i in range(11): \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if i == 0:\n",
    "            predicts = model.forwardPred(testData)\n",
    "        else:\n",
    "            predicts = finetunnedModels[i-1].forwardPred(testData)\n",
    "\n",
    "    \n",
    "    predictLabels = [[list(labels.keys())[i] for i in j if list(labels.keys())[i] != PADDING_TOKEN and list(labels.keys())[i] != START_TOKEN and list(labels.keys())[i] != STOP_TOKEN] for j in predicts]\n",
    "       \n",
    "        \n",
    "    saveToIob2(testWords, predictLabels, \"../Results/\" + str(i) + \".iob2\")\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feec807-97fe-4bef-b411-2b6580a7b64a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a625b66f05a6716aa62bf8e78e9986d1cdda2644917a6be9f119c99e84bc87a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
