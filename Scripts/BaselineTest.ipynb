{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3999f4dd",
   "metadata": {},
   "source": [
    "Just using this to write the easily test the code for the baseline model. Final implementation will be in a py script, so it can be run from command line using GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa3f03",
   "metadata": {},
   "source": [
    "# To do!\n",
    "- create function to extract data to train model -- DONE!\n",
    "- create function to output tags into appropriate format -- DONE!\n",
    "- make model -- DONE!\n",
    "  - Incorporate start, stop and unknown tokens into the convert data shape. Start and stop should be both a label and a vocab. Unknown should only be vocab -- DONE!\n",
    "  - Define allowed transitions, such as cannot transition into start token, cannot transition into pad token, except from stop token, cannot transition out of stop token except into pad token, can only transition into I tokens, from the B token of the same category. Potentially use allowed_transitions from the allen nlp CRF module to create it, it should then be fed into the model on its creation -- DONE!\n",
    "- define hyperparamter space and random space search to optimize on dev dataset\n",
    "  - Hyperparameters we have are DIM_EMBEDDING, LSTM_HIDDEN, LEARNING_RATE, EPOCHS and BATCH_SIZE. The values we have currently were selected arbitrarily, we could look at articles implementing Bi-LSTM and CRF for inspiration on ranges and appropriate values. \n",
    "  - https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html I think this might be the easiest way to implement it, otherwise we might have to implement from scratch\n",
    "- train model -- This part should be working, just need to select the hyperparameters before we actually do it.\n",
    "- submit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786fed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for ray\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torch.nn.parallel import DataParallel\n",
    "import os\n",
    "from ray.air import Checkpoint, session\n",
    "# TODO: Migrate to ray.train.Checkpoint and remove following line(not sure how to do it)\n",
    "os.environ[\"RAY_AIR_NEW_PERSISTENCE_MODE\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9eefc47-bbb1-4b41-9e7f-5ef731463776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24e10924350>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Putting all the imports in one place for readability\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from allennlp.modules.conditional_random_field import ConditionalRandomField as CRF\n",
    "from allennlp.modules import conditional_random_field as CRFmodule\n",
    "from torcheval.metrics.functional import multiclass_accuracy\n",
    "from torcheval.metrics.functional import multiclass_confusion_matrix as MCM\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Setting seeds to ensure reproducibility of results\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a67f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts the data into 2 lists of lists, one with the tokens another with the tags\n",
    "\n",
    "\n",
    "def extractData(filePath):\n",
    "    \"\"\"\n",
    "    Returns:tuple: A tuple containing input data (list of lists of words), tags (list of lists of tags),\n",
    "    and metadata (list of tuples containing newdoc_id, sent_id, and text).\n",
    "    \"\"\"\n",
    "    wordsData = []\n",
    "    tagsData = []\n",
    "    currentSent = None\n",
    "    with open(filePath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# sent_id\"):\n",
    "                sentId = line.split(\"= \")[1]\n",
    "            elif line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line:                \n",
    "                parts = line.split('\\t')\n",
    "                word = parts[1]\n",
    "                tag = parts[2]\n",
    "                if sentId != currentSent:\n",
    "                    currentSent = sentId\n",
    "                    wordsData.append([word])\n",
    "                    tagsData.append([tag])\n",
    "                else:\n",
    "                    wordsData[-1].append(word)\n",
    "                    tagsData[-1].append(tag)\n",
    "    return wordsData, tagsData\n",
    "\n",
    "# Example usage:\n",
    "#file_path = \"../Data/UniversalNER/train/en_ewt-ud-train.iob2\"\n",
    "#words_data, tags_data = extract_data(file_path)\n",
    "# for words, tags in zip(words_data, tags_data):\n",
    "#     print(\"Words:\", words)\n",
    "#     print(\"Tags:\", tags)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "380bc832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  7,  8, 10, 11,  9,  6,  4,  2],\n",
      "        [ 1,  6,  5,  2,  0,  0,  0,  0,  0]])\n",
      "tensor([[1, 5, 5, 5, 5, 5, 3, 5, 2],\n",
      "        [1, 3, 4, 2, 0, 0, 0, 0, 0]])\n",
      "{'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3, '?': 4, 'Falls': 5, 'Iguazu': 6, 'Where': 7, 'in': 8, 'is': 9, 'the': 10, 'world': 11}\n",
      "{'<PAD>': 0, '<START>': 1, '<END>': 2, 'B-LOC': 3, 'I-LOC': 4, 'O': 5}\n"
     ]
    }
   ],
   "source": [
    "#Converts the Data into a tensor for use by the model\n",
    "\n",
    "def convertDataShape(data, vocabulary = {}, labels = [], training = True, PADDING_TOKEN = '<PAD>', START_TOKEN = '<START>', STOP_TOKEN = '<END>', UNKNOWN_TOKEN = '<UNK>'):\n",
    "    \"\"\"\n",
    "    If training is enabled creates a vocabulary of all words in a list. Otherwise, a vocabulary should be passed.\n",
    "    Does the same with the labels.\n",
    "    Creates a matrix of sentences and positions, where each value indicates a word via its index in the vocabulary.\n",
    "    Creates another matrix of sentences and positions, where the values indicate a label.\n",
    "    '<PAD>' or another user defined token is used as padding for short sentences. Will also act as an unknown token, if not training, it is assumed to be in vocabulary.\n",
    "    Returns, the vocabulary, the labels and the two matrices.\n",
    "    \n",
    "    Input:\n",
    "    data          - (string list * string list) list - List of sentences. Each sentence is a tuple of two lists. The first is a list of words, the second a list of labels.\n",
    "    vocabulary    - string : int dictionary          - Dictionary of words in the vocabulary, values are the indices. Should be provided if not training. Defaults to empty dict.\n",
    "    labels        - string : int dictionary          - Dictionary of labels to classify, values are the indices. Should be provided if not training. Defaults to empty dict.\n",
    "    training      - boolean                          - Boolean variable deffining whether training is taking place, if yes then a new vocabulary will be created. Defaults to yes.\n",
    "    PADDING_TOKEN - string                           - Token to be used as padding. Default is provided\n",
    "    START_TOKEN   - string                           - Token to be used as marker for the start of the sentence. Default is provided\n",
    "    STOP_TOKEN    - string                           - Token to be used as marker for the end of the sentence. Default is provided\n",
    "    UNKNOWN_TOKEN - string                           - Token to be used as the unknown token. Default is provided\n",
    "    \n",
    "    Output:\n",
    "    Xmatrix       - 2D torch.tensor                  - 2d torch tensor containing the index of the word in the sentence in the vocabulary\n",
    "    Ymatrix       - 2D torch.tensor                  - 2d torch tensor containing the index of the label in the sentence in the labels\n",
    "    vocabulary    - string : int dictionary          - Dictionary of words, with indices as values, used for training.\n",
    "    labels        - string : int dictionary          - Dictionary of all the labels, with indices as values, used for classification. (all the labels are expected to be present in the training data, or in other words, the label list provided should be exhaustive)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if training:\n",
    "        vocabList = sorted(set(word for sentence, _ in data for word in sentence))\n",
    "        \n",
    "        #In order to be able to work with unknown words in the future, we turn some of the least common words into unknown words so we can train on them\n",
    "        #This is done by removing them from the vocab list before creating the dictionary\n",
    "        vocabCount = Counter([word for sentence, _ in data for word in sentence])\n",
    "        UNKNOWN_RATIO = 5 #This should be percentage of tokens we want to turn into Unknown tokens, the least common tokens will be used\n",
    "        cutoff = int(len(vocabList) / (100 / UNKNOWN_RATIO)) + 1\n",
    "        removeList = vocabCount.most_common()[:-cutoff:-1]\n",
    "        for i in removeList:\n",
    "            vocabList.remove(i[0])\n",
    "\n",
    "        # Adding the special tokens in the first positions after the least common have been removed and creating the dictionaries\n",
    "        vocabList = [PADDING_TOKEN, START_TOKEN, STOP_TOKEN, UNKNOWN_TOKEN] + vocabList\n",
    "        vocabulary = {word: i for i, word in enumerate(vocabList)}\n",
    "        labelList = [PADDING_TOKEN, START_TOKEN, STOP_TOKEN] + sorted(set(label for _, sentenceLabels in data for label in sentenceLabels))\n",
    "        labels = {label: i for i, label in enumerate(labelList)}\n",
    "    \n",
    "    # Adding two to the max len in order to accomodate the introduction of start and end tokens\n",
    "    maxLen = max(len(sentence) for sentence, _ in data) + 2\n",
    "    Xmatrix = np.zeros((len(data), maxLen), dtype=int)\n",
    "    Ymatrix = np.zeros((len(data), maxLen), dtype=int)\n",
    "\n",
    "    for i, (sentence, sentenceLabels) in enumerate(data):\n",
    "        #Set the first token as the start token (assumes it's index is 1)\n",
    "        Xmatrix[i, 0] = 1\n",
    "        Ymatrix[i, 0] = 1\n",
    "        #Set all the indices to the correct index, with the unknown token as default\n",
    "        for j, word in enumerate(sentence):\n",
    "            Xmatrix[i, j+1] = vocabulary.get(word, vocabulary[UNKNOWN_TOKEN])\n",
    "        for j, label in enumerate(sentenceLabels):\n",
    "            Ymatrix[i, j+1] = labels.get(label, labels[START_TOKEN])\n",
    "            lastWord = j         \n",
    "        # Sets the token after the last word as en end token\n",
    "        Xmatrix[i, lastWord + 2] = 2\n",
    "        Ymatrix[i, lastWord + 2] = 2\n",
    "    \n",
    "    return torch.tensor(Xmatrix, dtype=torch.long), torch.tensor(Ymatrix, dtype=torch.long), vocabulary, labels\n",
    "\n",
    "# two first sentences of EWT training dataset so that quickdebugging can be run\n",
    "\n",
    "\n",
    "\n",
    "trainingDebugSen = [[\"Where\", \"in\", \"the\", \"world\", \"is\", \"Iguazu\", \"?\"], [\"Iguazu\", \"Falls\"]]\n",
    "trainingDebugTags = [[\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"], [\"B-LOC\", \"I-LOC\"]]\n",
    "\n",
    "dataDebug, labelsDebug, vocabDebug, tagsDebug = convertDataShape(list(zip(trainingDebugSen, trainingDebugTags)))\n",
    "print(dataDebug)\n",
    "print(labelsDebug)\n",
    "print(vocabDebug)\n",
    "print(tagsDebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cb63731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baselineModel(torch.nn.Module):\n",
    "    def __init__(self, nWords, tags, dimEmbed, dimHidden, constraints):\n",
    "        super().__init__()\n",
    "        self.dimEmbed = dimEmbed\n",
    "        self.dimHidden = dimHidden\n",
    "        self.vocabSize = nWords\n",
    "        self.tagSetSize = len(tags)\n",
    "\n",
    "        self.embed = nn.Embedding(nWords, dimEmbed)\n",
    "        self.LSTM = nn.LSTM(dimEmbed, dimHidden, bidirectional=True)\n",
    "        self.linear = nn.Linear(dimHidden * 2, self.tagSetSize)\n",
    "        \n",
    "\n",
    "        # Initialize the CRF layer\n",
    "        self.CRF = CRF(self.tagSetSize, constraints = constraints, include_start_end_transitions=True)\n",
    "\n",
    "    def forwardTrain(self, inputData, labels):\n",
    "        # Embedding and LSTM layers\n",
    "        wordVectors = self.embed(inputData)\n",
    "        lstmOut, _ = self.LSTM(wordVectors)\n",
    "        \n",
    "        # Linear layer\n",
    "        emissions = self.linear(lstmOut)\n",
    "        \n",
    "        # CRF layer to compute the log likelihood loss\n",
    "        log_likelihood = self.CRF(emissions, labels)\n",
    "        \n",
    "        # The loss is the negative log-likelihood\n",
    "        loss = -log_likelihood\n",
    "        return loss\n",
    "        \n",
    "    def forwardPred(self, inputData):\n",
    "        # Embedding and LSTM layers\n",
    "        wordVectors = self.embed(inputData)\n",
    "        lstmOut, _ = self.LSTM(wordVectors)\n",
    "        \n",
    "        # Linear layer\n",
    "        emissions = self.linear(lstmOut)\n",
    "        \n",
    "        # Decode the best path\n",
    "        best_paths = self.CRF.viterbi_tags(emissions)\n",
    "        \n",
    "        # Extract the predicted tags from the paths\n",
    "        predictions = [path for path, score in best_paths]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665c82f7-6c93-469b-9913-7483ed57ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saveToIob2(words, labels, outputFilePath):\n",
    "    \"\"\"\n",
    "    Save words and their corresponding labels in IOB2 format.\n",
    "\n",
    "    Args:\n",
    "    words (list): List of lists containing words.\n",
    "    labels (list): List of lists containing labels.\n",
    "    output_file (str): Path to the output IOB2 file.\n",
    "    \"\"\"\n",
    "    with open(outputFilePath, 'w', encoding='utf-8') as file:\n",
    "        for i in range(len(words)):\n",
    "            for j in range(len(words[i])):\n",
    "                line = f\"{j+1}\\t{words[i][j]}\\t{labels[i][j]}\\n\"\n",
    "                file.write(line)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d166f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two first sentences of EWT training dataset so that quickdebugging can be run\n",
    "\n",
    "tags = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\"]\n",
    "\n",
    "trainingDebugSen = [[\"Where\", \"in\", \"the\", \"world\", \"is\", \"Iguazu\", \"?\"], [\"Iguazu\", \"Falls\"]]\n",
    "trainingDebugTags = [[\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"], [\"B-LOC\", \"I-LOC\"]]\n",
    "\n",
    "dataDebug, labelsDebug, vocabDebug, tagsDebug = convertDataShape(list(zip(trainingDebugSen, trainingDebugTags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6192fac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 33.8591194152832\n",
      "Epoch 1, Loss: 24.502079010009766\n",
      "Epoch 2, Loss: 17.171268463134766\n",
      "Epoch 3, Loss: 11.20111083984375\n",
      "Epoch 4, Loss: 6.78648567199707\n"
     ]
    }
   ],
   "source": [
    "#Quick traininig script on the debug dataset\n",
    "\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)\n",
    "\n",
    "constraint_type = None\n",
    "\n",
    "model = baselineModel(len(vocabDebug), tagsDebug, DIM_EMBEDDING, LSTM_HIDDEN, constraint_type)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model.forwardTrain(dataDebug, labelsDebug)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f68fee95-753b-4a42-b2ce-cae881316205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting predicitons and checking accuracy\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictsDebug = model.forwardPred(dataDebug)\n",
    "\n",
    "confMat = MCM(torch.flatten(torch.tensor(predictsDebug, dtype=torch.long)), torch.flatten(labelsDebug), num_classes = len(tagsDebug))\n",
    "\n",
    "acc = torch.trace(confMat[1:,1:])/torch.sum(confMat[1:,1:]) #Taking away the first collumn and first row, because those correspond to the padding token and we don't care\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e1c711c-55e7-48e1-8bd9-b6d1c6ed3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the training data sets\n",
    "\n",
    "filePathTrain = \"../Data/UniversalNER/train/\"\n",
    "wordsData = []\n",
    "tagsData = []\n",
    "datasets = [\"da_ddt\", \"en_ewt\", \"hr_set\", \"pt_bosque\", \"sk_snk\", \"sr_set\", \"sv_talbanken\", \"zh_gsdsimp\", \"zh_gsd\"]\n",
    "\n",
    "for i in datasets:\n",
    "    wordsDataTemp, tagsDataTemp = extractData(filePathTrain + i + \"-ud-train.iob2\")\n",
    "    wordsData += wordsDataTemp\n",
    "    tagsData += tagsDataTemp\n",
    "\n",
    "trainData, trainLabels, vocab, labels = convertDataShape(list(zip(wordsData, tagsData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4168f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(config, data_dir=None):\n",
    "    net = baselineModel(len(vocabDebug), tagsDebug, DIM_EMBEDDING, LSTM_HIDDEN, constraint_type)\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs]\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
    "\n",
    "        session.report(\n",
    "            {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "            checkpoint=checkpoint,\n",
    "        )\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1add8989-9183-426d-97c3-b0b3f5e66093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 76.76655578613281\n",
      "Epoch 1, Loss: 45.305023193359375\n",
      "Epoch 2, Loss: 42.572418212890625\n",
      "Epoch 3, Loss: 42.45745849609375\n",
      "Epoch 4, Loss: 41.4228515625\n"
     ]
    }
   ],
   "source": [
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "PADDING_TOKEN = '<PAD>'\n",
    "START_TOKEN = '<START>'\n",
    "STOP_TOKEN = '<END>'\n",
    "# The make constraint from the module was yielding some weird results so I decided to hardcode this for our use case, assuming the following dict of tags\n",
    "#{'<PAD>': 0, '<START>': 1, '<END>': 2, '-': 3, 'B-LOC': 4, 'B-ORG': 5, 'B-PER': 6, 'I-LOC': 7, 'I-ORG': 8, 'I-PER': 9, 'O': 10}\n",
    "CONSTRAINTS = [(1, 4), (1, 5), (1, 6), (1, 10), (2, 0), (4, 2), (4, 4), (4, 5), (4, 6), (4, 7), (4, 10), \n",
    "              (5, 2), (5, 4), (5, 5), (5, 6), (5, 8), (5, 10), (6, 2), (6, 4), (6, 5), (6, 6), (6, 9), (6, 10),\n",
    "              (7, 2), (7, 4), (7, 5), (7, 6), (7, 7), (7, 10), (8, 2), (8, 4), (8, 5), (8, 6), (8, 8), (8, 10),\n",
    "              (9, 2), (9, 4), (9, 5), (9, 6), (9, 9), (9, 10), (10, 2), (10, 4), (10, 5), (10, 6), (10, 10)]\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)\n",
    "\n",
    "numBatches = trainData.shape[0] // BATCH_SIZE\n",
    "\n",
    "trainDataBatches = trainData[:BATCH_SIZE*numBatches].view(numBatches, trainData.shape[1], BATCH_SIZE)\n",
    "trainLabelsBatches = trainLabels[:BATCH_SIZE*numBatches].view(numBatches, trainLabels.shape[1], BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "model = baselineModel(len(vocab), labels, DIM_EMBEDDING, LSTM_HIDDEN, CONSTRAINTS)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    for batch in zip(trainDataBatches, trainLabelsBatches): \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model.forwardTrain(batch[0], batch[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "     \n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a832c348-e6bb-4692-8b23-c779baf641ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading all the dev datasets\n",
    "\n",
    "filePathDev = \"../Data/UniversalNER/dev/\"\n",
    "\n",
    "wordsDataDev = []\n",
    "tagsDataDev = []\n",
    "datasets = [\"da_ddt\", \"en_ewt\", \"hr_set\", \"pt_bosque\", \"sk_snk\", \"sr_set\", \"sv_talbanken\", \"zh_gsdsimp\", \"zh_gsd\"]\n",
    "\n",
    "for i in datasets:\n",
    "    wordsDataTemp, tagsDataTemp = extractData(filePathDev + i + \"-ud-dev.iob2\")\n",
    "    wordsDataDev += wordsDataTemp\n",
    "    tagsDataDev += tagsDataTemp\n",
    "\n",
    "devData, devLabels, _, _ = convertDataShape(list(zip(wordsData, tagsData)), vocabulary = vocab, labels = labels, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "247287fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 17:59:56,552\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "2024-03-26 17:59:59,411\tINFO tune.py:613 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tracked actor is not managed by this event manager: <TrackedActor 302833676580458670291854003854672144260>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\tune\\tune.py:1001\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mis_finished() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m-> 1001\u001b[0m     \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_verbosity(Verbosity\u001b[38;5;241m.\u001b[39mV1_EXPERIMENT):\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:686\u001b[0m, in \u001b[0;36mTuneController.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;66;03m# Handle one event\u001b[39;00m\n\u001b[1;32m--> 686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;66;03m# If there are no actors running, warn about potentially\u001b[39;00m\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;66;03m# insufficient resources\u001b[39;00m\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_manager\u001b[38;5;241m.\u001b[39mnum_live_actors:\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\air\\execution\\_internal\\actor_manager.py:191\u001b[0m, in \u001b[0;36mRayActorManager.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# We always try to start actors as this won't trigger an event callback\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_start_actors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# If an actor was killed, this was our event, and we return.\u001b[39;00m\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\air\\execution\\_internal\\actor_manager.py:361\u001b[0m, in \u001b[0;36mRayActorManager._try_start_actors\u001b[1;34m(self, max_actors)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;66;03m# Start Ray actor\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m actor \u001b[38;5;241m=\u001b[39m remote_actor_cls\u001b[38;5;241m.\u001b[39mremote(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# Track\u001b[39;00m\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\actor.py:830\u001b[0m, in \u001b[0;36mActorClass.options.<locals>.ActorOptionWrapper.remote\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremote\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m actor_cls\u001b[38;5;241m.\u001b[39m_remote(args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mupdated_options)\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\_private\\auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m auto_init_ray()\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py:388\u001b[0m, in \u001b[0;36m_tracing_actor_creation.<locals>._invocation_actor_class_remote_span\u001b[1;34m(self, args, kwargs, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ray_trace_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, args, kwargs, \u001b[38;5;241m*\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n\u001b[0;32m    390\u001b[0m class_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__ray_metadata__\u001b[38;5;241m.\u001b[39mclass_name\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\actor.py:1038\u001b[0m, in \u001b[0;36mActorClass._remote\u001b[1;34m(self, args, kwargs, **actor_options)\u001b[0m\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;66;03m# After serialize / deserialize modified class, the __module__\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;66;03m# of modified class will be ray.cloudpickle.cloudpickle.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# So, here pass actor_creation_function_descriptor to make\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;66;03m# sure export actor class correct.\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m     \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_actor_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_actor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodified_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_creation_function_descriptor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod_meta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m resources \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mresources_from_ray_options(actor_options)\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\_private\\function_manager.py:529\u001b[0m, in \u001b[0;36mFunctionActorManager.export_actor_class\u001b[1;34m(self, Class, actor_creation_function_descriptor, actor_method_names)\u001b[0m\n\u001b[0;32m    520\u001b[0m actor_class_info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: actor_creation_function_descriptor\u001b[38;5;241m.\u001b[39mclass_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m: actor_creation_function_descriptor\u001b[38;5;241m.\u001b[39mmodule_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactor_method_names\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mlist\u001b[39m(actor_method_names)),\n\u001b[0;32m    527\u001b[0m }\n\u001b[1;32m--> 529\u001b[0m \u001b[43mcheck_oversized_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_class_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_class_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker\u001b[38;5;241m.\u001b[39mgcs_client\u001b[38;5;241m.\u001b[39minternal_kv_put(\n\u001b[0;32m    537\u001b[0m     key, pickle\u001b[38;5;241m.\u001b[39mdumps(actor_class_info), \u001b[38;5;28;01mTrue\u001b[39;00m, KV_NAMESPACE_FUNCTION_TABLE\n\u001b[0;32m    538\u001b[0m )\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\_private\\utils.py:758\u001b[0m, in \u001b[0;36mcheck_oversized_function\u001b[1;34m(pickled, name, obj_type, worker)\u001b[0m\n\u001b[0;32m    747\u001b[0m error \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is too large (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m MiB > FUNCTION_SIZE_ERROR_THRESHOLD=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m MiB). Check that its definition is not implicitly capturing a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    756\u001b[0m     ray_constants\u001b[38;5;241m.\u001b[39mFUNCTION_SIZE_ERROR_THRESHOLD \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m),\n\u001b[0;32m    757\u001b[0m )\n\u001b[1;32m--> 758\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error)\n",
      "\u001b[1;31mValueError\u001b[0m: The actor ImplicitFunc is too large (254 MiB > FUNCTION_SIZE_ERROR_THRESHOLD=95 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 59\u001b[0m\n\u001b[0;32m     53\u001b[0m reporter \u001b[38;5;241m=\u001b[39m CLIReporter(metric_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Define dev dataset\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# dev_data = load_dev_data()\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Perform hyperparameter tuning\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_reporter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreporter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial_dirname_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Get best hyperparameters and corresponding results\u001b[39;00m\n\u001b[0;32m     68\u001b[0m best_config \u001b[38;5;241m=\u001b[39m analysis\u001b[38;5;241m.\u001b[39mget_best_config(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\tune\\tune.py:1008\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[0;32m   1006\u001b[0m             _report_air_progress(runner, air_progress_reporter)\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m-> 1008\u001b[0m     \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m tune_taken \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tune_start\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:1976\u001b[0m, in \u001b[0;36mTuneController.cleanup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1974\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcleanup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1975\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Cleanup trials and callbacks.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cleanup_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_experiment_callbacks()\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:793\u001b[0m, in \u001b[0;36mTuneController._cleanup_trials\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m     trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_to_trial[tracked_actor]\n\u001b[0;32m    789\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheduling trial stop at end of experiment (trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtracked_actor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    792\u001b[0m     )\n\u001b[1;32m--> 793\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schedule_trial_stop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;66;03m# Clean up cached actors now\u001b[39;00m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cleanup_cached_actors(force_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:1404\u001b[0m, in \u001b[0;36mTuneController._schedule_trial_stop\u001b[1;34m(self, trial, exception)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_to_trial\u001b[38;5;241m.\u001b[39mpop(tracked_actor)\n\u001b[0;32m   1402\u001b[0m trial\u001b[38;5;241m.\u001b[39mset_ray_actor(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1404\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remove_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracked_actor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracked_actor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:812\u001b[0m, in \u001b[0;36mTuneController._remove_actor\u001b[1;34m(self, tracked_actor)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_remove_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tracked_actor: TrackedActor):\n\u001b[1;32m--> 812\u001b[0m     stop_future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule_actor_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtracked_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_future\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m     now \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_manager\u001b[38;5;241m.\u001b[39mremove_actor(\n\u001b[0;32m    818\u001b[0m         tracked_actor, kill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stop_future\u001b[38;5;241m=\u001b[39mstop_future\n\u001b[0;32m    819\u001b[0m     ):\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# If the actor was previously alive, track\u001b[39;00m\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\ray\\air\\execution\\_internal\\actor_manager.py:727\u001b[0m, in \u001b[0;36mRayActorManager.schedule_actor_task\u001b[1;34m(self, tracked_actor, method_name, args, kwargs, on_result, on_error, _return_future)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracked_actor \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_live_actors_to_ray_actors_resources:\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;66;03m# Actor is not started, yet\u001b[39;00m\n\u001b[0;32m    726\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tracked_actor \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pending_actors_to_attrs:\n\u001b[1;32m--> 727\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    728\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTracked actor is not managed by this event manager: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    729\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtracked_actor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    730\u001b[0m         )\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# Cache tasks for future execution\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pending_actors_to_enqueued_actor_tasks[tracked_actor]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    734\u001b[0m         (tracked_actor_task, method_name, args, kwargs)\n\u001b[0;32m    735\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Tracked actor is not managed by this event manager: <TrackedActor 302833676580458670291854003854672144260>"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter search space\n",
    "config = {\n",
    "    \"DIM_EMBEDDING\": tune.choice([50, 100, 200]),\n",
    "    \"LSTM_HIDDEN\": tune.choice([25, 50, 100]),\n",
    "    \"LEARNING_RATE\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"EPOCHS\": tune.choice([3, 5, 7]),\n",
    "    \"BATCH_SIZE\": tune.choice([16, 32, 64])\n",
    "}\n",
    "\n",
    "# Define training function\n",
    "def train_model(config, dev_data):\n",
    "    DIM_EMBEDDING = config[\"DIM_EMBEDDING\"]\n",
    "    LSTM_HIDDEN = config[\"LSTM_HIDDEN\"]\n",
    "    LEARNING_RATE = config[\"LEARNING_RATE\"]\n",
    "    EPOCHS = config[\"EPOCHS\"]\n",
    "    BATCH_SIZE = config[\"BATCH_SIZE\"]\n",
    "\n",
    "    # Load dev data and preprocess if necessary\n",
    "    # dev_data = load_dev_data()\n",
    "\n",
    "    # Define your model, optimizer, and data loading here\n",
    "    model = baselineModel(len(vocab), labels, DIM_EMBEDDING, LSTM_HIDDEN, CONSTRAINTS)\n",
    "    model = DataParallel(model)  # Utilize DataParallel for multi-GPU training\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        for batch in dev_data: \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = model.module.forwardTrain(batch[0], batch[1])  # Access the module inside DataParallel\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # Return performance metric for optimization (e.g., accuracy, F1 score)\n",
    "    return 0.9  # Dummy return value for demonstration\n",
    "\n",
    "# Initialize Ray Tune\n",
    "ray.init()\n",
    "\n",
    "# Define scheduler and reporter\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    "    metric=\"accuracy\",  # Specify the metric parameter to optimize\n",
    "    mode=\"max\"  # Specify the mode parameter\n",
    ")\n",
    "reporter = CLIReporter(metric_columns=[\"accuracy\"])\n",
    "\n",
    "# Define dev dataset\n",
    "# dev_data = load_dev_data()\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "analysis = tune.run(\n",
    "    lambda config: train_model(config, devData),\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    trial_dirname_creator=lambda trial: str(trial))\n",
    "\n",
    "# Get best hyperparameters and corresponding results\n",
    "best_config = analysis.get_best_config(metric=\"accuracy\")\n",
    "best_accuracy = analysis.get_best_trial(metric=\"accuracy\").last_result[\"accuracy\"]\n",
    "\n",
    "print(\"Best config:\", best_config)\n",
    "print(\"Best accuracy:\", best_accuracy)\n",
    "\n",
    "# Shut down Ray Tune\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "541d3ae7-5508-4f97-aa7e-3e26f163d71d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m devDataBatches:\n\u001b[1;32m---> 13\u001b[0m         predicts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforwardPred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m, in \u001b[0;36mbaselineModel.forwardPred\u001b[1;34m(self, inputData)\u001b[0m\n\u001b[0;32m     38\u001b[0m emissions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(lstmOut)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Decode the best path\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m best_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCRF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviterbi_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43memissions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Extract the predicted tags from the paths\u001b[39;00m\n\u001b[0;32m     44\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [path \u001b[38;5;28;01mfor\u001b[39;00m path, score \u001b[38;5;129;01min\u001b[39;00m best_paths]\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\allennlp\\modules\\conditional_random_field\\conditional_random_field.py:444\u001b[0m, in \u001b[0;36mConditionalRandomField.viterbi_tags\u001b[1;34m(self, logits, mask, top_k)\u001b[0m\n\u001b[0;32m    441\u001b[0m tag_sequence[sequence_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, end_tag] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# We pass the tags and the transitions to `viterbi_decode`.\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m viterbi_paths, viterbi_scores \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviterbi_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransition_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m top_k_paths \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m viterbi_path, viterbi_score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(viterbi_paths, viterbi_scores):\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;66;03m# Get rid of START and END sentinels and append.\u001b[39;00m\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\allennlp\\nn\\util.py:537\u001b[0m, in \u001b[0;36mviterbi_decode\u001b[1;34m(tag_sequence, transition_matrix, tag_observations, allowed_start_transitions, allowed_end_transitions, top_k)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Best pairwise potential path score from the previous timestep.\u001b[39;00m\n\u001b[0;32m    536\u001b[0m max_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(summed_potentials\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], top_k)\n\u001b[1;32m--> 537\u001b[0m scores, paths \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummed_potentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;66;03m# If we have an observation for this timestep, use it\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# instead of the distribution over tags.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m observation \u001b[38;5;241m=\u001b[39m tag_observations[timestep]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Getting predicitons and checking accuracy\n",
    "\n",
    "DEV_BATCH_SIZE = 200\n",
    "\n",
    "devNumBatches = devData.shape[0] // DEV_BATCH_SIZE\n",
    "devDataBatches = devData[:DEV_BATCH_SIZE*devNumBatches].view(devNumBatches, devData.shape[1], DEV_BATCH_SIZE)\n",
    "devLabelsBatches = devLabels[:DEV_BATCH_SIZE*devNumBatches].view(devNumBatches, devLabels.shape[1], DEV_BATCH_SIZE)\n",
    "\n",
    "predicts = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in devDataBatches:\n",
    "        predicts += model.forwardPred(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2b2e38-ea62-4183-ab94-76caefe031b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `input` and `target` should have the same first dimension, got shapes torch.Size([16330400]) and torch.Size([16379868]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m confMat \u001b[38;5;241m=\u001b[39m MCM(torch\u001b[38;5;241m.\u001b[39mflatten(torch\u001b[38;5;241m.\u001b[39mtensor(predicts, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)), torch\u001b[38;5;241m.\u001b[39mflatten(devLabels), num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Taking away the first three collumns and rows, because those correspond to the functional tokens and we don't care\u001b[39;00m\n\u001b[0;32m      4\u001b[0m acc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtrace(confMat[\u001b[38;5;241m3\u001b[39m:,\u001b[38;5;241m3\u001b[39m:])\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39msum(confMat[\u001b[38;5;241m3\u001b[39m:,\u001b[38;5;241m3\u001b[39m:]) \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torcheval\\metrics\\functional\\classification\\confusion_matrix.py:148\u001b[0m, in \u001b[0;36mmulticlass_confusion_matrix\u001b[1;34m(input, target, num_classes, normalize)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03mCompute multi-class confusion matrix, a matrix of dimension num_classes x num_classes where each element at position `(i,j)` is the number of examples with true class `i` that were predicted to be class `j`.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mSee also :func:`binary_confusion_matrix <torcheval.metrics.functional.binary_confusion_matrix>`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m            [0, 0, 0, 1]])\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    147\u001b[0m _confusion_matrix_param_check(num_classes, normalize)\n\u001b[1;32m--> 148\u001b[0m sparse_cm \u001b[38;5;241m=\u001b[39m _confusion_matrix_update(\u001b[38;5;28minput\u001b[39m, target, num_classes)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _confusion_matrix_compute(sparse_cm, normalize\u001b[38;5;241m=\u001b[39mnormalize)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torcheval\\metrics\\functional\\classification\\confusion_matrix.py:215\u001b[0m, in \u001b[0;36m_confusion_matrix_update\u001b[1;34m(input, target, num_classes)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_confusion_matrix_update\u001b[39m(\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor, target: torch\u001b[38;5;241m.\u001b[39mTensor, num_classes: \u001b[38;5;28mint\u001b[39m\n\u001b[0;32m    214\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 215\u001b[0m     _confusion_matrix_update_input_check(\u001b[38;5;28minput\u001b[39m, target, num_classes)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _update(\u001b[38;5;28minput\u001b[39m, target, num_classes)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torcheval\\metrics\\functional\\classification\\confusion_matrix.py:253\u001b[0m, in \u001b[0;36m_confusion_matrix_update_input_check\u001b[1;34m(input, target, num_classes)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_confusion_matrix_update_input_check\u001b[39m(\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    249\u001b[0m     target: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    250\u001b[0m     num_classes: Optional[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    251\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    254\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `input` and `target` should have the same first dimension, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m         )\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    260\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget should be a one-dimensional tensor, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The `input` and `target` should have the same first dimension, got shapes torch.Size([16330400]) and torch.Size([16379868])."
     ]
    }
   ],
   "source": [
    "confMat = MCM(torch.flatten(torch.tensor(predicts, dtype=torch.long)), torch.flatten(devLabels), num_classes = len(labels))\n",
    "\n",
    "#Taking away the first three collumns and rows, because those correspond to the functional tokens and we don't care\n",
    "acc = torch.trace(confMat[3:,3:])/torch.sum(confMat[3:,3:]) \n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb634faa-297e-4321-bd66-f3737af3a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilePath = \"./baselineModel.iob2\"\n",
    "\n",
    "#convert the predictions back into labels\n",
    "\n",
    "# creates a list of lists of tags, where the padding token is excluded\n",
    "predictLabels = [[list(labels.keys())[i] for i in j if list(labels.keys())[i] != PADDING_TOKEN and list(labels.keys())[i] != START_TOKEN and list(labels.keys())[i] != STOP_TOKEN] for j in predicts]\n",
    "\n",
    "# the saveToIob2 works when provided data in the right format\n",
    "saveToIob2(devWordsData, predictLabels, outputFilePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run thes every time after running ray if it crashed and didn't turn down\n",
    "# Shut down Ray Tune\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7bae6b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 18:57:12,541\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "2024-03-26 18:57:14,738\tINFO tune.py:613 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (504 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
      "== Status ==\n",
      "Current time: 2024-03-26 18:58:34 (running for 00:01:15.59)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Logical resource usage: 10.0/12 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/danii/AppData/Local/Temp/ray/session_2024-03-26_18-57-09_907373_28344/artifacts/2024-03-26_18-57-16/lambda_2024-03-26_18-57-16/driver_artifacts\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-03-26 18:58:39 (running for 00:01:20.65)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Logical resource usage: 10.0/12 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/danii/AppData/Local/Temp/ray/session_2024-03-26_18-57-09_907373_28344/artifacts/2024-03-26_18-57-16/lambda_2024-03-26_18-57-16/driver_artifacts\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-03-26 18:58:44 (running for 00:01:25.77)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Logical resource usage: 10.0/12 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: C:/Users/danii/AppData/Local/Temp/ray/session_2024-03-26_18-57-09_907373_28344/artifacts/2024-03-26_18-57-16/lambda_2024-03-26_18-57-16/driver_artifacts\n",
      "Number of trials: 10/10 (10 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2024-03-26 18:58:47,142 E 3916 24004] (gcs_server.exe) logging.cc:97: Unhandled exception: class std::bad_alloc. what(): bad allocation\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define hyperparameter search space\n",
    "config = {\n",
    "    \"DIM_EMBEDDING\": tune.choice([50, 100, 200]),\n",
    "    \"LSTM_HIDDEN\": tune.choice([25, 50, 100]),\n",
    "    \"LEARNING_RATE\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"EPOCHS\": tune.choice([3, 5, 7]),\n",
    "    \"BATCH_SIZE\": tune.choice([16, 32, 64])\n",
    "}\n",
    "\n",
    "ray_constants.FUNCTION_SIZE_ERROR_THRESHOLD = 967619810\n",
    "\n",
    "def train_model(config, trainData, trainLabels):\n",
    "    DIM_EMBEDDING = config[\"DIM_EMBEDDING\"]\n",
    "    LSTM_HIDDEN = config[\"LSTM_HIDDEN\"]\n",
    "    LEARNING_RATE = config[\"LEARNING_RATE\"]\n",
    "    EPOCHS = config[\"EPOCHS\"]\n",
    "    BATCH_SIZE = config[\"BATCH_SIZE\"]\n",
    "    CONSTRAINTS = [(1, 4), (1, 5), (1, 6), (1, 10), (2, 0), (4, 2), (4, 4), (4, 5), (4, 6), (4, 7), (4, 10), \n",
    "                (5, 2), (5, 4), (5, 5), (5, 6), (5, 8), (5, 10), (6, 2), (6, 4), (6, 5), (6, 6), (6, 9), (6, 10),\n",
    "                (7, 2), (7, 4), (7, 5), (7, 6), (7, 7), (7, 10), (8, 2), (8, 4), (8, 5), (8, 6), (8, 8), (8, 10),\n",
    "                (9, 2), (9, 4), (9, 5), (9, 6), (9, 9), (9, 10), (10, 2), (10, 4), (10, 5), (10, 6), (10, 10)]\n",
    "    \n",
    "    numBatches = trainData.shape[0] // BATCH_SIZE\n",
    "\n",
    "    trainDataBatches = trainData[:BATCH_SIZE*numBatches].view(numBatches, trainData.shape[1], BATCH_SIZE)\n",
    "    trainLabelsBatches = trainLabels[:BATCH_SIZE*numBatches].view(numBatches, trainLabels.shape[1], BATCH_SIZE)\n",
    "\n",
    "    # Define your model, optimizer, and data loading here\n",
    "    model = baselineModel(len(vocab), labels, DIM_EMBEDDING, LSTM_HIDDEN, CONSTRAINTS)\n",
    "    #model = DataParallel(model)  # Utilize DataParallel for multi-GPU training\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        for batch in zip(trainDataBatches, trainLabelsBatches): \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = model.module.forwardTrain(batch[0], batch[1])  # Access the module inside DataParallel\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        tune.report(loss=loss.item())\n",
    "\n",
    "# Initialize Ray Tune\n",
    "ray.init()\n",
    "\n",
    "# Define scheduler and reporter\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    "    metric=\"loss\",  # Specify the metric parameter\n",
    "    mode=\"min\")  # Specify the mode parameter\n",
    "reporter = CLIReporter(metric_columns=[\"loss\", \"training_iteration\"])\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "analysis = tune.run(\n",
    "    lambda config: train_model(config, trainData, trainLabels),\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    trial_dirname_creator=lambda trial: str(trial))\n",
    "\n",
    "# Get best hyperparameters and corresponding results\n",
    "best_config = analysis.get_best_config(metric=\"loss\")\n",
    "best_loss = analysis.get_best_trial(metric=\"loss\").last_result[\"loss\"]\n",
    "\n",
    "print(\"Best config:\", best_config)\n",
    "print(\"Best loss:\", best_loss)\n",
    "\n",
    "# Shut down Ray Tune\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a625b66f05a6716aa62bf8e78e9986d1cdda2644917a6be9f119c99e84bc87a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
