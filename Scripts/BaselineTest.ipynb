{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3999f4dd",
   "metadata": {},
   "source": [
    "Just using this to write the easily test the code for the baseline model. Final implementation will be in a py script, so it can be run from command line using GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa3f03",
   "metadata": {},
   "source": [
    "# To do!\n",
    "- create function to extract data to train model DONE!\n",
    "- create function to output tags into appropriate format DONE!\n",
    "- make model\n",
    "  - Incorporate start, stop and unknown tokens into the convert data shape. Start and stop should be both a label and a vocab. Unknown should only be vocab\n",
    "  - Define allowed transitions, such as cannot transition into start token, cannot transition into pad token, except from stop token, cannot transition out of stop token except into pad token, can only transition into I tokens, from the B token of the same category. Potentially use allowed_transitions from the allen nlp CRF module to create it, it should then be fed into the model on its creation\n",
    "- train model This part should be working, but need the rest\n",
    "- define hyperparamter space and random space search to optimize on dev dataset\n",
    "- submit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b9eefc47-bbb1-4b41-9e7f-5ef731463776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x196c3525ed0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Putting all the imports in one place for readability\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from allennlp.modules.conditional_random_field import ConditionalRandomField as CRF\n",
    "from allennlp.modules.conditional_random_field import allowed_transitions\n",
    "from torcheval.metrics.functional import multiclass_accuracy\n",
    "from torcheval.metrics.functional import multiclass_confusion_matrix as MCM\n",
    "import random\n",
    "\n",
    "\n",
    "# Setting seeds to ensure reproducibility of results\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a67f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts the data into 2 lists of lists, one with the tokens another with the tags\n",
    "\n",
    "\n",
    "def extractData(filePath):\n",
    "    \"\"\"\n",
    "    Returns:tuple: A tuple containing input data (list of lists of words), tags (list of lists of tags),\n",
    "    and metadata (list of tuples containing newdoc_id, sent_id, and text).\n",
    "    \"\"\"\n",
    "    wordsData = []\n",
    "    tagsData = []\n",
    "    metadata = []\n",
    "    currentSent = None\n",
    "    with open(filePath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# newdoc id\"):\n",
    "                newdocId = line.split(\"= \")[1]\n",
    "            elif line.startswith(\"# sent_id\"):\n",
    "                sentId = line.split(\"= \")[1]\n",
    "            elif line.startswith(\"# text\"):\n",
    "                text = line.split(\"= \")[1]\n",
    "            elif line:\n",
    "                parts = line.split('\\t')\n",
    "                word = parts[1]\n",
    "                tag = parts[2]\n",
    "                if sentId != currentSent:\n",
    "                    currentSent = sentId\n",
    "                    wordsData.append([word])\n",
    "                    tagsData.append([tag])\n",
    "                    metadata.append((newdocId, sentId, text))\n",
    "                else:\n",
    "                    wordsData[-1].append(word)\n",
    "                    tagsData[-1].append(tag)\n",
    "    return wordsData, tagsData, metadata\n",
    "\n",
    "# Example usage:\n",
    "#file_path = \"../Data/UniversalNER/train/en_ewt-ud-train.iob2\"\n",
    "#words_data, tags_data, metadata = extract_data(file_path)\n",
    "# for words, tags, meta in zip(words_data, tags_data, metadata):\n",
    "#     print(\"Words:\", words)\n",
    "#     print(\"Tags:\", tags)\n",
    "#     print(\"Metadata:\", meta)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "380bc832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  7,  8, 10, 11,  9,  6,  4,  2],\n",
      "        [ 1,  6,  5,  2,  0,  0,  0,  0,  0]])\n",
      "tensor([[1, 6, 6, 6, 6, 6, 4, 6, 0],\n",
      "        [1, 4, 5, 0, 0, 0, 0, 0, 0]])\n",
      "{'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNKNOWN>': 3, '?': 4, 'Falls': 5, 'Iguazu': 6, 'Where': 7, 'in': 8, 'is': 9, 'the': 10, 'world': 11}\n",
      "{'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNKNOWN>': 3, 'B-LOC': 4, 'I-LOC': 5, 'O': 6}\n"
     ]
    }
   ],
   "source": [
    "#Converts the Data into a tensor for use by the model\n",
    "\n",
    "def convertDataShape(data, vocabulary={}, labels=[], training=True, paddingToken='<PAD>', START_TOKEN = '<START>', STOP_TOKEN = '<END>', UNKNOWN_TOKEN = '<UNKNOWN>'):\n",
    "    \"\"\"\n",
    "    If training is enabled creates a vocabulary of all words in a list. Otherwise, a vocabulary should be passed.\n",
    "    Does the same with the labels.\n",
    "    Creates a matrix of sentences and positions, where each value indicates a word via its index in the vocabulary.\n",
    "    Creates another matrix of sentences and positions, where the values indicate a label.\n",
    "    '<PAD>' or another user defined token is used as padding for short sentences. Will also act as an unknown token, if not training, it is assumed to be in vocabulary.\n",
    "    Returns, the vocabulary, the labels and the two matrices.\n",
    "    \n",
    "    Input:\n",
    "    data         - (string list * string list) list - List of sentences. Each sentence is a tuple of two lists. The first is a list of words, the second a list of labels.\n",
    "    vocabulary   - string : int dictionary          - Dictionary of words in the vocabulary, values are the indices. Should be provided if not training. Defaults to empty dict.\n",
    "    labels       - string : int dictionary          - Dictionary of labels to classify, values are the indices. Should be provided if not training. Defaults to empty dict.\n",
    "    training     - boolean                          - Boolean variable deffining whether training is taking place, if yes then a new vocabulary will be created. Defaults to yes.\n",
    "    paddingToken - string                           - Token to be used as padding and unknown. Default is provided\n",
    "    \n",
    "    Output:\n",
    "    Xmatrix      - 2D torch.tensor                  - 2d torch tensor containing the index of the word in the sentence in the vocabulary\n",
    "    Ymatrix      - 2D torch.tensor                  - 2d torch tensor containing the index of the label in the sentence in the labels\n",
    "    vocabulary   - string : int dictionary          - Dictionary of words, with indices as values, used for training.\n",
    "    labels       - string : int dictionary          - Dictionary of all the labels, with indices as values, used for classification. (all the labels are expected to be present in the training data, or in other words, the label list provided should be exhaustive)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if training:\n",
    "        vocabList = [paddingToken, START_TOKEN, STOP_TOKEN, UNKNOWN_TOKEN] + sorted(set(word for sentence, _ in data for word in sentence))\n",
    "        labelList = [paddingToken, START_TOKEN, STOP_TOKEN, UNKNOWN_TOKEN] + sorted(set(label for _, sentence_labels in data for label in sentence_labels))\n",
    "        vocabulary = {word: i for i, word in enumerate(vocabList)}\n",
    "        labels = {label: i for i, label in enumerate(labelList)}\n",
    "    \n",
    "    max_len = max(len(sentence) for sentence, _ in data) + 2\n",
    "    Xmatrix = np.zeros((len(data), max_len), dtype=int)\n",
    "    Ymatrix = np.zeros((len(data), max_len), dtype=int)\n",
    "\n",
    "    for i, (sentence, sentence_labels) in enumerate(data):\n",
    "        Xmatrix[i, 0] = 1\n",
    "        Ymatrix[i, 0] = 1\n",
    "        for j, word in enumerate(sentence):\n",
    "            Xmatrix[i, j+1] = vocabulary.get(word, vocabulary[paddingToken])\n",
    "            last_word = j\n",
    "        Xmatrix[i, last_word+2] = 2\n",
    "        for j, label in enumerate(sentence_labels):\n",
    "            Ymatrix[i, j+1] = labels.get(label, labels[paddingToken])\n",
    "            last_word = j\n",
    "        Xmatrix[i, last_word+2] = 2\n",
    "    \n",
    "    return torch.tensor(Xmatrix, dtype=torch.long), torch.tensor(Ymatrix, dtype=torch.long), vocabulary, labels\n",
    "\n",
    "# two first sentences of EWT training dataset so that quickdebugging can be run\n",
    "\n",
    "tags = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\"]\n",
    "\n",
    "trainingDebugSen = [[\"Where\", \"in\", \"the\", \"world\", \"is\", \"Iguazu\", \"?\"], [\"Iguazu\", \"Falls\"]]\n",
    "trainingDebugTags = [[\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"], [\"B-LOC\", \"I-LOC\"]]\n",
    "\n",
    "dataDebug, labelsDebug, vocabDebug, tagsDebug = convertDataShape(list(zip(trainingDebugSen, trainingDebugTags)))\n",
    "print(dataDebug)\n",
    "print(labelsDebug)\n",
    "print(vocabDebug)\n",
    "print(tagsDebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5cb63731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class baselineModel(torch.nn.Module):\n",
    "    def __init__(self, nWords, tags, dimEmbed, dimHidden, constraint_type=None):\n",
    "        super().__init__()\n",
    "        self.dimEmbed = dimEmbed\n",
    "        self.dimHidden = dimHidden\n",
    "        self.vocabSize = nWords\n",
    "        self.tagSetSize = len(tags)\n",
    "\n",
    "        self.embed = nn.Embedding(nWords, dimEmbed)\n",
    "        self.LSTM = nn.LSTM(dimEmbed, dimHidden, bidirectional=True)\n",
    "        self.linear = nn.Linear(dimHidden * 2, self.tagSetSize)\n",
    "        \n",
    "        # \n",
    "        if constraint_type:\n",
    "            tags_reversed = {v: k for k, v in tags.items()}\n",
    "            constraints = allowed_transitions(constraint_type, tags_reversed)\n",
    "        else:\n",
    "            constraints = None\n",
    "\n",
    "        # Initialize the CRF layer\n",
    "        self.CRF = CRF(self.tagSetSize, constraints=constraints, include_start_end_transitions=True)\n",
    "\n",
    "    def forwardTrain(self, inputData, labels):\n",
    "        # Embedding and LSTM layers\n",
    "        wordVectors = self.embed(inputData)\n",
    "        lstmOut, _ = self.LSTM(wordVectors)\n",
    "        \n",
    "        # Linear layer\n",
    "        emissions = self.linear(lstmOut)\n",
    "        \n",
    "        # CRF layer to compute the log likelihood loss\n",
    "        log_likelihood = self.CRF(emissions, labels)\n",
    "        \n",
    "        # The loss is the negative log-likelihood\n",
    "        loss = -log_likelihood\n",
    "        return loss\n",
    "        \n",
    "    def forwardPred(self, inputData):\n",
    "        # Embedding and LSTM layers\n",
    "        wordVectors = self.embed(inputData)\n",
    "        lstmOut, _ = self.LSTM(wordVectors)\n",
    "        \n",
    "        # Linear layer\n",
    "        emissions = self.linear(lstmOut)\n",
    "        \n",
    "        # Decode the best path\n",
    "        best_paths = self.CRF.viterbi_tags(emissions)\n",
    "        \n",
    "        # Extract the predicted tags from the paths\n",
    "        predictions = [path for path, score in best_paths]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665c82f7-6c93-469b-9913-7483ed57ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saveToIob2(words, labels, outputFilePath):\n",
    "    \"\"\"\n",
    "    Save words and their corresponding labels in IOB2 format.\n",
    "\n",
    "    Args:\n",
    "    words (list): List of lists containing words.\n",
    "    labels (list): List of lists containing labels.\n",
    "    output_file (str): Path to the output IOB2 file.\n",
    "    \"\"\"\n",
    "    with open(outputFilePath, 'w', encoding='utf-8') as file:\n",
    "        for i in range(len(words)):\n",
    "            for j in range(len(words[i])):\n",
    "                line = f\"{j+1}\\t{words[i][j]}\\t{labels[i][j]}\\n\"\n",
    "                file.write(line)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d166f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two first sentences of EWT training dataset so that quickdebugging can be run\n",
    "\n",
    "tags = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\"]\n",
    "\n",
    "trainingDebugSen = [[\"Where\", \"in\", \"the\", \"world\", \"is\", \"Iguazu\", \"?\"], [\"Iguazu\", \"Falls\"]]\n",
    "trainingDebugTags = [[\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"], [\"B-LOC\", \"I-LOC\"]]\n",
    "\n",
    "dataDebug, labelsDebug, vocabDebug, tagsDebug = convertDataShape(list(zip(trainingDebugSen, trainingDebugTags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6192fac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 36.203163146972656\n",
      "Epoch 1, Loss: 26.62387466430664\n",
      "Epoch 2, Loss: 18.93437957763672\n",
      "Epoch 3, Loss: 12.457599639892578\n",
      "Epoch 4, Loss: 7.5099945068359375\n"
     ]
    }
   ],
   "source": [
    "#Quick traininig script on the debug dataset\n",
    "\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)\n",
    "\n",
    "constraint_type = \"BIOUL\"\n",
    "\n",
    "model = baselineModel(len(vocabDebug), tagsDebug, DIM_EMBEDDING, LSTM_HIDDEN, constraint_type)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model.forwardTrain(dataDebug, labelsDebug)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f68fee95-753b-4a42-b2ce-cae881316205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5455)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting predicitons and checking accuracy\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictsDebug = model.forwardPred(dataDebug)\n",
    "\n",
    "confMat = MCM(torch.flatten(torch.tensor(predictsDebug, dtype=torch.long)), torch.flatten(labelsDebug), num_classes = len(tagsDebug))\n",
    "\n",
    "acc = torch.trace(confMat[1:,1:])/torch.sum(confMat[1:,1:]) #Taking away the first collumn and first row, because those correspond to the padding token and we don't care\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1e1c711c-55e7-48e1-8bd9-b6d1c6ed3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the EWT train data set\n",
    "\n",
    "filePath = \"../Data/UniversalNER/train/en_ewt-ud-train.iob2\"\n",
    "wordsData, tagsData, metadata = extractData(filePath)\n",
    "\n",
    "trainData, trainLabels, vocab, labels = convertDataShape(list(zip(wordsData, tagsData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1add8989-9183-426d-97c3-b0b3f5e66093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 0, Loss: 188.17529296875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforwardTrain(batch[\u001b[38;5;241m0\u001b[39m], batch[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 32\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(epoch)  \n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "PADDING_TOKEN = '<PAD>'\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)\n",
    "\n",
    "numBatches = trainData.shape[0] // BATCH_SIZE\n",
    "\n",
    "trainDataBatches = trainData[:BATCH_SIZE*numBatches].view(numBatches, trainData.shape[1], BATCH_SIZE)\n",
    "trainLabelsBatches = trainLabels[:BATCH_SIZE*numBatches].view(numBatches, trainLabels.shape[1], BATCH_SIZE)\n",
    "\n",
    "\n",
    "constraint_type = \"BIO\"\n",
    "\n",
    "model = baselineModel(len(vocab), labels, DIM_EMBEDDING, LSTM_HIDDEN, constraint_type)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    for batch in zip(trainDataBatches, trainLabelsBatches): \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model.forwardTrain(batch[0], batch[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    print(epoch)  \n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a832c348-e6bb-4692-8b23-c779baf641ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dev EWT dataset\n",
    "\n",
    "devFilePath = \"../Data/UniversalNER/dev/en_ewt-ud-dev.iob2\"\n",
    "devWordsData, devTagsData, metadata = extractData(devFilePath)\n",
    "\n",
    "devData, devLabels, _, _ = convertDataShape(list(zip(devWordsData, devTagsData)), vocabulary = vocab, labels = labels, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "541d3ae7-5508-4f97-aa7e-3e26f163d71d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Getting predicitons and checking accuracy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m     predicts \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforwardPred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m confMat \u001b[38;5;241m=\u001b[39m MCM(torch\u001b[38;5;241m.\u001b[39mflatten(torch\u001b[38;5;241m.\u001b[39mtensor(predicts, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)), torch\u001b[38;5;241m.\u001b[39mflatten(devLabels), num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels))\n\u001b[0;32m      9\u001b[0m acc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtrace(confMat[\u001b[38;5;241m1\u001b[39m:,\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39msum(confMat[\u001b[38;5;241m1\u001b[39m:,\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;66;03m#Taking away the first collumn and first row, because those correspond to the padding token and we don't care\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[83], line 40\u001b[0m, in \u001b[0;36mbaselineModel.forwardPred\u001b[1;34m(self, inputData)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforwardPred\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputData):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Embedding and LSTM layers\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     wordVectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     lstmOut, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLSTM(wordVectors)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Linear layer\u001b[39;00m\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\School\\ITU\\Coding\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "#Getting predicitons and checking accuracy\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicts = model.forwardPred(devData)\n",
    "\n",
    "confMat = MCM(torch.flatten(torch.tensor(predicts, dtype=torch.long)), torch.flatten(devLabels), num_classes = len(labels))\n",
    "\n",
    "acc = torch.trace(confMat[1:,1:])/torch.sum(confMat[1:,1:]) #Taking away the first collumn and first row, because those correspond to the padding token and we don't care\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fb634faa-297e-4321-bd66-f3737af3a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilePath = \"./baselineModel.iob2\"\n",
    "\n",
    "#convert the predictions back into labels\n",
    "\n",
    "# creates a list of lists of tags, where the padding token is excluded\n",
    "predictLabels = [[list(labels.keys())[i] for i in j if list(labels.keys())[i] != PADDING_TOKEN] for j in predicts]\n",
    "\n",
    "# the saveToIob2 works when provided data in the right format\n",
    "saveToIob2(devWordsData, devTagsData, outputFilePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57991999-714a-4b13-9cc8-f2580608a4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a625b66f05a6716aa62bf8e78e9986d1cdda2644917a6be9f119c99e84bc87a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
