{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3999f4dd",
   "metadata": {},
   "source": [
    "Just using this to write and easily test the code for the baseline model. Final implementation will be in a py script, so it can be run from command line using GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa3f03",
   "metadata": {},
   "source": [
    "# To do!\n",
    "- create function to extract data to train model\n",
    "- create function to output tags into appropriate format\n",
    "- make model - Sort of done\n",
    "- make sure model works\n",
    "- train model\n",
    "- define hyperparamter space and random space search to optimize on dev dataset\n",
    "- submit results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "389911a3-72a6-465e-bd9b-a654c2bdac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDataShape(data, vocabulary = {}, labels = [], training = True, paddingToken = '<PAD>'):\n",
    "    \"\"\"\n",
    "    If training is enabled creates a vocabulary of all words in a list. Otherwise, a vocabulary should be passed.\n",
    "    Does the same with the labels.\n",
    "    Creates a matrix of sentences and positions, where each value indicates a word via its index in the vocabulary.\n",
    "    Creates another matrix of sentences and positions, where the values indicate a label.\n",
    "    '<PAD>' or another user defined token is used as padding for short sentences. Will also act as an unknown token, if not training, it is assumed to be in vocabulary.\n",
    "    Returns, the vocabulary, the labels and the two matrices.\n",
    "    \n",
    "    Input:\n",
    "    data         - (string list * string list) list - List of sentences. Each sentence is a tuple of two lists. The first is a list of words, the second a list of labels.\n",
    "    vocabulary   - string : int dictionary          - Dictionary of words in the vocabulary, values are the indices. Should be provided if not training. Defaults to empty dict.\n",
    "    labels       - string list                      - List of labels to classify. Should be provided if not training. Defaults to empty list.\n",
    "    training     - boolean                          - Boolean variable deffining whether training is taking place, if yes then a new vocabulary will be created. Defaults to yes.\n",
    "    paddingToken - string                           - Token to be used as padding and unknown. Default is provided\n",
    "    \n",
    "    Output:\n",
    "    Xmatrix      - 2D np.array                      - 2d numpy array containing the index of the word in the sentence in the vocabulary\n",
    "    Ymatrix      - 2D np.array                      - 2d numpy array containing the index of the label in the sentence in the labels\n",
    "    vocabulary   - string : int dictionary          - Dictionary of words, with indices as values, used for training.\n",
    "    labels       - string list                      - List of all the labels used for classification. (all the labels are expected to be present in the training data, or in other words, the label list provided should be exhaustive)\n",
    "    \"\"\"\n",
    "        \n",
    "    if training:\n",
    "        if paddingToken not in vocabulary:\n",
    "            vocabulary[paddingToken] = 0\n",
    "        vocabList = list(vocabulary.keys())\n",
    "        if paddingToken not in labels:\n",
    "            labels.append(paddingToken)\n",
    "        for i in data:\n",
    "            for j in i[0]:\n",
    "                if j not in vocabulary:\n",
    "                    vocabList.append(j)\n",
    "                    vocabulary[j] = vocabList.index(j)\n",
    "            for j in i[1]:\n",
    "                if j not in labels:\n",
    "                    labels.append(j)\n",
    "                    \n",
    "    Xmatrix = np.zeros((len(data), max([len(x[0]) for x in data])))\n",
    "    Ymatrix = np.zeros((len(data), max([len(x[0]) for x in data])))\n",
    "    \n",
    "    for i, sentence in zip(range(len(data)), data): #this implementation assumes that the padding Token has value 0 in the vocab dictionary and is in index 0 of the labels list\n",
    "        for j, word in zip(range(len(sentence[0])), sentence[0]):\n",
    "            if word in vocabulary:\n",
    "                Xmatrix[i][j] = vocabulary[word]\n",
    "            else:\n",
    "                Xmatrix[i][j] = vocabulary[paddingToken]\n",
    "        for j, label in zip(range(len(sentence[1])), sentence[1]):\n",
    "            Ymatrix[i][j] = labels.index(label)\n",
    "    \n",
    "    return Xmatrix, Ymatrix, vocabulary, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f708d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import TorchCRF\n",
    "from torch import nn\n",
    "\n",
    "#Creating the class for the baseline Model\n",
    "\n",
    "class baselineModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, nWords, tags, dimEmbed, dimHidden):\n",
    "        super().__init__()\n",
    "        self.dimEmbed = dimEmbed\n",
    "        self.dimHidden = dimHidden\n",
    "        self.vocabSize = nWords\n",
    "        self.tagSetSize = len(tags)\n",
    "        self.tagSet = tags\n",
    "\n",
    "        self.embed = nn.Embedding(nWords, dimEmbed)\n",
    "        self.LSTM = nn.LSTM(dimEmbed, dimHidden, batch_first = True, bidirectional = True)\n",
    "        self.linear = nn.Linear(dimHidden * 2, self.tagSetSize) \n",
    "        \n",
    "        self.CRF = TorchCRF.CRF(self.tagSetSize, batch_first = True)\n",
    "\n",
    "\n",
    "        \n",
    "    def forwardTrain(self, inputData):\n",
    "        wordVectors = self.embed(inputData)\n",
    "        out, _ = self.LSTM(wordVectors.view((inputData.shape[0], inputData.shape[1], self.dimEmbed)))\n",
    "        out = self.linear(out)\n",
    "        preds = self.CRF.decode(out)\n",
    "        loss = self.CRF.forward(out, preds)\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def forwardPred(self, inputData):\n",
    "        wordVectors = self.embed(inputData)\n",
    "        out, _ = self.LSTM(wordVectors.view((inputData.shape[0], inputData.shape[1], self.dimEmbed)))\n",
    "        out = self.linear(out)\n",
    "        preds = self.CRF.decode(out)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e56125b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two first sentences of EWT training dataset so that quickdebugging can be run\n",
    "\n",
    "tags = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\"]\n",
    "\n",
    "trainingDebugSen = [[\"Where\", \"in\", \"the\", \"world\", \"is\", \"Iguazu\", \"?\"], [\"Iguazu\", \"Falls\"]]\n",
    "trainingDebugTags = [[\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"], [\"B-LOC\", \"I-LOC\"]]\n",
    "\n",
    "data, labels, vocab, tags = convertDataShape(list(zip(trainingDebugSen, trainingDebugTags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aea07922-c414-4a57-ab4d-0835817e72a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\TorchCRF\\__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorCompare.cpp:519.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforwardTrain(torch\u001b[38;5;241m.\u001b[39mtensor(data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong))\n\u001b[0;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m, in \u001b[0;36mbaselineModel.forwardTrain\u001b[1;34m(self, inputData)\u001b[0m\n\u001b[0;32m     29\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n\u001b[0;32m     30\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCRF\u001b[38;5;241m.\u001b[39mdecode(out)\n\u001b[1;32m---> 31\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCRF\u001b[38;5;241m.\u001b[39mforward(out, preds)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\TorchCRF\\__init__.py:90\u001b[0m, in \u001b[0;36mCRF.forward\u001b[1;34m(self, emissions, tags, mask, reduction)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     65\u001b[0m         emissions: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m         reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     69\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m        reduction is ``none``, ``()`` otherwise.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(emissions, tags\u001b[38;5;241m=\u001b[39mtags, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_mean\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid reduction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\TorchCRF\\__init__.py:154\u001b[0m, in \u001b[0;36mCRF._validate\u001b[1;34m(self, emissions, tags, mask)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected last dimension of emissions is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tags\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memissions\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m emissions\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m tags\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    156\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe first two dimensions of emissions and tags must match, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(emissions\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(tags\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "\n",
    "model = baselineModel(len(vocab), tags, DIM_EMBEDDING, LSTM_HIDDEN)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    model.zero_grad()\n",
    "     \n",
    "    optimizer.zero_grad()\n",
    "    loss = model.forwardTrain(torch.tensor(data, dtype=torch.long))\n",
    "        \n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    \n",
    "    print(epoch)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25479923-1370-426d-a97f-5f416bd17bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
