{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3999f4dd",
   "metadata": {},
   "source": [
    "Just using this to write the easily test the code for the baseline model. Final implementation will be in a py script, so it can be run from command line using GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa3f03",
   "metadata": {},
   "source": [
    "# To do!\n",
    "- create function to extract data to train model -- DONE!\n",
    "- create function to output tags into appropriate format -- DONE!\n",
    "- make model -- DONE!\n",
    "  - Incorporate start, stop and unknown tokens into the convert data shape. Start and stop should be both a label and a vocab. Unknown should only be vocab -- DONE!\n",
    "  - Define allowed transitions, such as cannot transition into start token, cannot transition into pad token, except from stop token, cannot transition out of stop token except into pad token, can only transition into I tokens, from the B token of the same category. Potentially use allowed_transitions from the allen nlp CRF module to create it, it should then be fed into the model on its creation -- DONE!\n",
    "- define hyperparamter space and random space search to optimize on dev dataset\n",
    "  - Hyperparameters we have are DIM_EMBEDDING, LSTM_HIDDEN, LEARNING_RATE, EPOCHS and BATCH_SIZE. The values we have currently were selected arbitrarily, we could look at articles implementing Bi-LSTM and CRF for inspiration on ranges and appropriate values. \n",
    "  - https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html I think this might be the easiest way to implement it, otherwise we might have to implement from scratch\n",
    "- train model -- This part should be working, just need to select the hyperparameters before we actually do it.\n",
    "- submit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786fed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for ray\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torch.nn.parallel import DataParallel\n",
    "import os\n",
    "#from ray.air import Checkpoint, session\n",
    "# TODO: Migrate to ray.train.Checkpoint and remove following line(not sure how to do it)\n",
    "os.environ[\"RAY_AIR_NEW_PERSISTENCE_MODE\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9eefc47-bbb1-4b41-9e7f-5ef731463776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2699a0b4e70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Putting all the imports in one place for readability\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from allennlp.modules.conditional_random_field import ConditionalRandomField as CRF\n",
    "from allennlp.modules import conditional_random_field as CRFmodule\n",
    "from torcheval.metrics.functional import multiclass_accuracy\n",
    "from torcheval.metrics.functional import multiclass_confusion_matrix as MCM\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Setting seeds to ensure reproducibility of results\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a67f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts the data into 2 lists of lists, one with the tokens another with the tags\n",
    "\n",
    "\n",
    "def extractData(filePath):\n",
    "    \"\"\"\n",
    "    Returns:tuple: A tuple containing input data (list of lists of words), tags (list of lists of tags),\n",
    "    and metadata (list of tuples containing newdoc_id, sent_id, and text).\n",
    "    \"\"\"\n",
    "    wordsData = []\n",
    "    tagsData = []\n",
    "    currentSent = None\n",
    "    with open(filePath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# sent_id\"):\n",
    "                sentId = line.split(\"= \")[1]\n",
    "            elif line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line:                \n",
    "                parts = line.split('\\t')\n",
    "                word = parts[1]\n",
    "                tag = parts[2]\n",
    "                if sentId != currentSent:\n",
    "                    currentSent = sentId\n",
    "                    wordsData.append([word])\n",
    "                    tagsData.append([tag])\n",
    "                else:\n",
    "                    wordsData[-1].append(word)\n",
    "                    tagsData[-1].append(tag)\n",
    "    return wordsData, tagsData\n",
    "\n",
    "# Example usage:\n",
    "#file_path = \"../Data/UniversalNER/train/en_ewt-ud-train.iob2\"\n",
    "#words_data, tags_data = extract_data(file_path)\n",
    "# for words, tags in zip(words_data, tags_data):\n",
    "#     print(\"Words:\", words)\n",
    "#     print(\"Tags:\", tags)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380bc832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  7,  8, 10, 11,  9,  6,  4,  2],\n",
      "        [ 1,  6,  5,  2,  0,  0,  0,  0,  0]])\n",
      "tensor([[1, 5, 5, 5, 5, 5, 3, 5, 2],\n",
      "        [1, 3, 4, 2, 0, 0, 0, 0, 0]])\n",
      "{'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3, '?': 4, 'Falls': 5, 'Iguazu': 6, 'Where': 7, 'in': 8, 'is': 9, 'the': 10, 'world': 11}\n",
      "{'<PAD>': 0, '<START>': 1, '<END>': 2, 'B-LOC': 3, 'I-LOC': 4, 'O': 5}\n"
     ]
    }
   ],
   "source": [
    "#Converts the Data into a tensor for use by the model\n",
    "\n",
    "def convertDataShape(data, vocabulary = {}, labels = [], training = True, PADDING_TOKEN = '<PAD>', START_TOKEN = '<START>', STOP_TOKEN = '<END>', UNKNOWN_TOKEN = '<UNK>'):\n",
    "    \"\"\"\n",
    "    If training is enabled creates a vocabulary of all words in a list. Otherwise, a vocabulary should be passed.\n",
    "    Does the same with the labels.\n",
    "    Creates a matrix of sentences and positions, where each value indicates a word via its index in the vocabulary.\n",
    "    Creates another matrix of sentences and positions, where the values indicate a label.\n",
    "    '<PAD>' or another user defined token is used as padding for short sentences. Will also act as an unknown token, if not training, it is assumed to be in vocabulary.\n",
    "    Returns, the vocabulary, the labels and the two matrices.\n",
    "    \n",
    "    Input:\n",
    "    data          - (string list * string list) list - List of sentences. Each sentence is a tuple of two lists. The first is a list of words, the second a list of labels.\n",
    "    vocabulary    - string : int dictionary          - Dictionary of words in the vocabulary, values are the indices. Should be provided if not training. Defaults to empty dict.\n",
    "    labels        - string : int dictionary          - Dictionary of labels to classify, values are the indices. Should be provided if not training. Defaults to empty dict.\n",
    "    training      - boolean                          - Boolean variable deffining whether training is taking place, if yes then a new vocabulary will be created. Defaults to yes.\n",
    "    PADDING_TOKEN - string                           - Token to be used as padding. Default is provided\n",
    "    START_TOKEN   - string                           - Token to be used as marker for the start of the sentence. Default is provided\n",
    "    STOP_TOKEN    - string                           - Token to be used as marker for the end of the sentence. Default is provided\n",
    "    UNKNOWN_TOKEN - string                           - Token to be used as the unknown token. Default is provided\n",
    "    \n",
    "    Output:\n",
    "    Xmatrix       - 2D torch.tensor                  - 2d torch tensor containing the index of the word in the sentence in the vocabulary\n",
    "    Ymatrix       - 2D torch.tensor                  - 2d torch tensor containing the index of the label in the sentence in the labels\n",
    "    vocabulary    - string : int dictionary          - Dictionary of words, with indices as values, used for training.\n",
    "    labels        - string : int dictionary          - Dictionary of all the labels, with indices as values, used for classification. (all the labels are expected to be present in the training data, or in other words, the label list provided should be exhaustive)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if training:\n",
    "        vocabList = sorted(set(word for sentence, _ in data for word in sentence))\n",
    "        \n",
    "        #In order to be able to work with unknown words in the future, we turn some of the least common words into unknown words so we can train on them\n",
    "        #This is done by removing them from the vocab list before creating the dictionary\n",
    "        vocabCount = Counter([word for sentence, _ in data for word in sentence])\n",
    "        UNKNOWN_RATIO = 5 #This should be percentage of tokens we want to turn into Unknown tokens, the least common tokens will be used\n",
    "        cutoff = int(len(vocabList) / (100 / UNKNOWN_RATIO)) + 1\n",
    "        removeList = vocabCount.most_common()[:-cutoff:-1]\n",
    "        for i in removeList:\n",
    "            vocabList.remove(i[0])\n",
    "\n",
    "        # Adding the special tokens in the first positions after the least common have been removed and creating the dictionaries\n",
    "        vocabList = [PADDING_TOKEN, START_TOKEN, STOP_TOKEN, UNKNOWN_TOKEN] + vocabList\n",
    "        vocabulary = {word: i for i, word in enumerate(vocabList)}\n",
    "        labelList = [PADDING_TOKEN, START_TOKEN, STOP_TOKEN] + sorted(set(label for _, sentenceLabels in data for label in sentenceLabels))\n",
    "        labels = {label: i for i, label in enumerate(labelList)}\n",
    "    \n",
    "    # Adding two to the max len in order to accomodate the introduction of start and end tokens\n",
    "    maxLen = max(len(sentence) for sentence, _ in data) + 2\n",
    "    Xmatrix = np.zeros((len(data), maxLen), dtype=int)\n",
    "    Ymatrix = np.zeros((len(data), maxLen), dtype=int)\n",
    "\n",
    "    for i, (sentence, sentenceLabels) in enumerate(data):\n",
    "        #Set the first token as the start token (assumes it's index is 1)\n",
    "        Xmatrix[i, 0] = 1\n",
    "        Ymatrix[i, 0] = 1\n",
    "        #Set all the indices to the correct index, with the unknown token as default\n",
    "        for j, word in enumerate(sentence):\n",
    "            Xmatrix[i, j+1] = vocabulary.get(word, vocabulary[UNKNOWN_TOKEN])\n",
    "        for j, label in enumerate(sentenceLabels):\n",
    "            Ymatrix[i, j+1] = labels.get(label, labels[START_TOKEN])\n",
    "            lastWord = j         \n",
    "        # Sets the token after the last word as an end token(assumes it's index is 2)\n",
    "        Xmatrix[i, lastWord + 2] = 2\n",
    "        Ymatrix[i, lastWord + 2] = 2\n",
    "    \n",
    "    return torch.tensor(Xmatrix, dtype=torch.long), torch.tensor(Ymatrix, dtype=torch.long), vocabulary, labels\n",
    "\n",
    "# two first sentences of EWT training dataset so that quickdebugging can be run\n",
    "\n",
    "\n",
    "\n",
    "trainingDebugSen = [[\"Where\", \"in\", \"the\", \"world\", \"is\", \"Iguazu\", \"?\"], [\"Iguazu\", \"Falls\"]]\n",
    "trainingDebugTags = [[\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"], [\"B-LOC\", \"I-LOC\"]]\n",
    "\n",
    "dataDebug, labelsDebug, vocabDebug, tagsDebug = convertDataShape(list(zip(trainingDebugSen, trainingDebugTags)))\n",
    "print(dataDebug)\n",
    "print(labelsDebug)\n",
    "print(vocabDebug)\n",
    "print(tagsDebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb63731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baselineModel(torch.nn.Module):\n",
    "    def __init__(self, nWords, tags, dimEmbed, dimHidden, constraints):\n",
    "        super().__init__()\n",
    "        self.dimEmbed = dimEmbed\n",
    "        self.dimHidden = dimHidden\n",
    "        self.vocabSize = nWords\n",
    "        self.tagSetSize = len(tags)\n",
    "\n",
    "        self.embed = nn.Embedding(nWords, dimEmbed)\n",
    "        self.LSTM = nn.LSTM(dimEmbed, dimHidden, bidirectional=True)\n",
    "        self.linear = nn.Linear(dimHidden * 2, self.tagSetSize)\n",
    "        \n",
    "\n",
    "        # Initialize the CRF layer\n",
    "        self.CRF = CRF(self.tagSetSize, constraints = constraints, include_start_end_transitions=True)\n",
    "\n",
    "    def forwardTrain(self, inputData, labels):\n",
    "        # Embedding and LSTM layers\n",
    "        wordVectors = self.embed(inputData)\n",
    "        lstmOut, _ = self.LSTM(wordVectors)\n",
    "        \n",
    "        # Linear layer\n",
    "        emissions = self.linear(lstmOut)\n",
    "        \n",
    "        # CRF layer to compute the log likelihood loss\n",
    "        log_likelihood = self.CRF(emissions, labels)\n",
    "        \n",
    "        # The loss is the negative log-likelihood\n",
    "        loss = -log_likelihood\n",
    "        return loss\n",
    "        \n",
    "    def forwardPred(self, inputData):\n",
    "        # Embedding and LSTM layers\n",
    "        wordVectors = self.embed(inputData)\n",
    "        lstmOut, _ = self.LSTM(wordVectors)\n",
    "        \n",
    "        # Linear layer\n",
    "        emissions = self.linear(lstmOut)\n",
    "        \n",
    "        # Decode the best path\n",
    "        best_paths = self.CRF.viterbi_tags(emissions)\n",
    "        \n",
    "        # Extract the predicted tags from the paths\n",
    "        predictions = [path for path, score in best_paths]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "665c82f7-6c93-469b-9913-7483ed57ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saveToIob2(words, labels, outputFilePath):\n",
    "    \"\"\"\n",
    "    Save words and their corresponding labels in IOB2 format.\n",
    "\n",
    "    Args:\n",
    "    words (list): List of lists containing words.\n",
    "    labels (list): List of lists containing labels.\n",
    "    output_file (str): Path to the output IOB2 file.\n",
    "    \"\"\"\n",
    "    with open(outputFilePath, 'w', encoding='utf-8') as file:\n",
    "        for i in range(len(words)):\n",
    "            for j in range(len(words[i])):\n",
    "                line = f\"{j+1}\\t{words[i][j]}\\t{labels[i][j]}\\n\"\n",
    "                file.write(line)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d166f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two first sentences of EWT training dataset so that quickdebugging can be run\n",
    "\n",
    "tags = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\"]\n",
    "\n",
    "trainingDebugSen = [[\"Where\", \"in\", \"the\", \"world\", \"is\", \"Iguazu\", \"?\"], [\"Iguazu\", \"Falls\"]]\n",
    "trainingDebugTags = [[\"O\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"], [\"B-LOC\", \"I-LOC\"]]\n",
    "\n",
    "dataDebug, labelsDebug, vocabDebug, tagsDebug = convertDataShape(list(zip(trainingDebugSen, trainingDebugTags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6192fac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 33.8591194152832\n",
      "Epoch 1, Loss: 24.502077102661133\n",
      "Epoch 2, Loss: 17.171268463134766\n",
      "Epoch 3, Loss: 11.20111083984375\n",
      "Epoch 4, Loss: 6.7864837646484375\n"
     ]
    }
   ],
   "source": [
    "#Quick traininig script on the debug dataset\n",
    "\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)\n",
    "\n",
    "constraint_type = None\n",
    "\n",
    "model = baselineModel(len(vocabDebug), tagsDebug, DIM_EMBEDDING, LSTM_HIDDEN, constraint_type)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model.forwardTrain(dataDebug, labelsDebug)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f68fee95-753b-4a42-b2ce-cae881316205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting predicitons and checking accuracy\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictsDebug = model.forwardPred(dataDebug)\n",
    "\n",
    "confMat = MCM(torch.flatten(torch.tensor(predictsDebug, dtype=torch.long)), torch.flatten(labelsDebug), num_classes = len(tagsDebug))\n",
    "\n",
    "acc = torch.trace(confMat[1:,1:])/torch.sum(confMat[1:,1:]) #Taking away the first collumn and first row, because those correspond to the padding token and we don't care\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "303feee8-389f-443c-8dd9-bcd27805c432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  7,  8, 10, 11,  9,  6,  4,  2],\n",
       "        [ 1,  6,  5,  2,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e1c711c-55e7-48e1-8bd9-b6d1c6ed3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the training data sets\n",
    "\n",
    "filePathTrain = \"../Data/UniversalNER/train/\"\n",
    "wordsData = []\n",
    "tagsData = []\n",
    "datasets = [\"da_ddt\", \"en_ewt\", \"hr_set\", \"pt_bosque\", \"sk_snk\", \"sr_set\", \"sv_talbanken\", \"zh_gsdsimp\", \"zh_gsd\"]\n",
    "\n",
    "for i in datasets:\n",
    "    wordsDataTemp, tagsDataTemp = extractData(filePathTrain + i + \"-ud-train.iob2\")\n",
    "    wordsData += wordsDataTemp\n",
    "    tagsData += tagsDataTemp\n",
    "\n",
    "trainData, trainLabels, vocab, labels = convertDataShape(list(zip(wordsData, tagsData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4168f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_cifar(config, data_dir=None):\n",
    "    net = baselineModel(len(vocabDebug), tagsDebug, DIM_EMBEDDING, LSTM_HIDDEN, constraint_type)\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs]\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
    "\n",
    "        session.report(\n",
    "            {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "            checkpoint=checkpoint,\n",
    "        )\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1add8989-9183-426d-97c3-b0b3f5e66093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 74.68666076660156\n",
      "Epoch 1, Loss: 46.97889709472656\n",
      "Epoch 2, Loss: 42.708892822265625\n",
      "Epoch 3, Loss: 40.866546630859375\n",
      "Epoch 4, Loss: 40.169830322265625\n"
     ]
    }
   ],
   "source": [
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "PADDING_TOKEN = '<PAD>'\n",
    "START_TOKEN = '<START>'\n",
    "STOP_TOKEN = '<END>'\n",
    "# The make constraint from the module was yielding some weird results so I decided to hardcode this for our use case, assuming the following dict of tags\n",
    "#{'<PAD>': 0, '<START>': 1, '<END>': 2, '-': 3, 'B-LOC': 4, 'B-ORG': 5, 'B-OTH': 6, 'B-PER': 7, 'I-LOC': 8, 'I-ORG': 9, 'I-OTH': 10, 'I-PER': 11, 'O': 12}\n",
    "CONSTRAINTS = [(1, 4), (1, 5), (1, 6), (1, 7), (1, 12), (2, 0), (4, 2), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 12),\n",
    "              (5, 2), (5, 4), (5, 5), (5, 6), (5, 7), (5, 9), (5, 12), (6, 2), (6, 4), (6, 5), (6, 6), (6, 7), (6, 10), (6, 12),\n",
    "              (7, 2), (7, 4), (7, 5), (7, 6), (7, 7), (7, 11), (7, 12), (8, 2), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 12),\n",
    "              (9, 2), (9, 4), (9, 5), (9, 6), (9, 7), (9, 9), (9, 12), (10, 2), (10, 4), (10, 5), (10, 6), (10, 7), (10, 10), (10, 12),\n",
    "              (11, 2), (11, 4), (11, 5), (11, 6), (11, 7), (11, 11), (11, 12), (12, 2), (12, 4), (12, 5), (12, 6), (12, 7), (12, 12)]\n",
    "\n",
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "torch.manual_seed(666)\n",
    "\n",
    "numBatches = trainData.shape[0] // BATCH_SIZE\n",
    "\n",
    "trainDataBatches = trainData[:BATCH_SIZE*numBatches].view(numBatches, trainData.shape[1], BATCH_SIZE)\n",
    "trainLabelsBatches = trainLabels[:BATCH_SIZE*numBatches].view(numBatches, trainLabels.shape[1], BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "model = baselineModel(len(vocab), labels, DIM_EMBEDDING, LSTM_HIDDEN, CONSTRAINTS)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    for batch in zip(trainDataBatches, trainLabelsBatches): \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model.forwardTrain(batch[0], batch[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "     \n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a832c348-e6bb-4692-8b23-c779baf641ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading all the dev datasets\n",
    "\n",
    "filePathDev = \"../Data/UniversalNER/dev/\"\n",
    "\n",
    "wordsDataDev = []\n",
    "tagsDataDev = []\n",
    "datasets = [\"da_ddt\", \"en_ewt\", \"hr_set\", \"pt_bosque\", \"sk_snk\", \"sr_set\", \"sv_talbanken\", \"zh_gsdsimp\", \"zh_gsd\"]\n",
    "\n",
    "for i in datasets:\n",
    "    wordsDataTemp, tagsDataTemp = extractData(filePathDev + i + \"-ud-dev.iob2\")\n",
    "    wordsDataDev += wordsDataTemp\n",
    "    tagsDataDev += tagsDataTemp\n",
    "\n",
    "devData, devLabels, _, _ = convertDataShape(list(zip(wordsDataDev, tagsDataDev)), vocabulary = vocab, labels = labels, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "541d3ae7-5508-4f97-aa7e-3e26f163d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions and checking accuracy\n",
    "\n",
    "DEV_BATCH_SIZE = 113\n",
    "\n",
    "devNumBatches = devData.shape[0] // DEV_BATCH_SIZE\n",
    "devDataBatches = devData[:DEV_BATCH_SIZE*devNumBatches].view(devNumBatches, DEV_BATCH_SIZE, devLabels.shape[1])\n",
    "devLabelsBatches = devLabels[:DEV_BATCH_SIZE*devNumBatches].view(devNumBatches, DEV_BATCH_SIZE, devLabels.shape[1])\n",
    "\n",
    "\n",
    "predicts = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in devDataBatches:\n",
    "        \n",
    "        predicts += model.forwardPred(batch)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab2b2e38-ea62-4183-ab94-76caefe031b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9085)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confMat = MCM(torch.flatten(torch.tensor(predicts, dtype=torch.long)), torch.flatten(devLabels), num_classes = len(labels))\n",
    "\n",
    "#Taking away the first three collumns and rows, because those correspond to the functional tokens and we don't care\n",
    "acc = torch.trace(confMat[3:,3:])/torch.sum(confMat[3:,3:]) \n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fb634faa-297e-4321-bd66-f3737af3a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilePath = \"./baselineModel.iob2\"\n",
    "\n",
    "#convert the predictions back into labels\n",
    "\n",
    "# creates a list of lists of tags, where the padding token is excluded\n",
    "predictLabels = [[list(labels.keys())[i] for i in j if list(labels.keys())[i] != PADDING_TOKEN and list(labels.keys())[i] != START_TOKEN and list(labels.keys())[i] != STOP_TOKEN] for j in predicts]\n",
    "\n",
    "# the saveToIob2 works when provided data in the right format\n",
    "saveToIob2(wordsDataDev, predictLabels, outputFilePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "79f9fb2b-c60d-49ba-8db1-ae44f48f91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the test dataset\n",
    "\n",
    "filePathTest = \"../Project/en_ewt-ud-test-masked.iob2\"\n",
    "\n",
    "wordsDataTest, tagsDataTest = extractData(filePathTest)\n",
    "\n",
    "testData, _, _, _ = convertDataShape(list(zip(wordsDataTest, tagsDataTest)), vocabulary = vocab, labels = labels, training = False)\n",
    "\n",
    "\n",
    "\n",
    "#getting the test predictions\n",
    "with torch.no_grad():\n",
    "\n",
    "    predictsTest = model.forwardPred(testData)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a0d7063-89a1-4426-a459-6fe75ae31540",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilePath = \"./baselineModelSubmit.iob2\"\n",
    "\n",
    "#convert the predictions back into labels\n",
    "\n",
    "# creates a list of lists of tags, where the padding token is excluded\n",
    "predictLabelsTest = [[list(labels.keys())[i] for i in j if list(labels.keys())[i] != PADDING_TOKEN and list(labels.keys())[i] != START_TOKEN and list(labels.keys())[i] != STOP_TOKEN] for j in predictsTest]\n",
    "\n",
    "# the saveToIob2 works when provided data in the right format\n",
    "saveToIob2(wordsDataTest, predictLabelsTest, outputFilePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7654c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run thes every time after running ray if it crashed and didn't turn down\n",
    "# Shut down Ray Tune\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bae6b5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['WMIC', 'PATH', 'Win32_VideoController', 'GET', 'AdapterCompatibility']' returned non-zero exit status 2147749890.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m         tune\u001b[38;5;241m.\u001b[39mreport(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Initialize Ray Tune\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m ray\u001b[38;5;241m.\u001b[39minit()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Define scheduler and reporter\u001b[39;00m\n\u001b[0;32m     52\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ASHAScheduler(\n\u001b[0;32m     53\u001b[0m     max_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     54\u001b[0m     grace_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     55\u001b[0m     reduction_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     56\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Specify the metric parameter\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Specify the mode parameter\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\ray\\_private\\worker.py:1567\u001b[0m, in \u001b[0;36minit\u001b[1;34m(address, num_cpus, num_gpus, resources, labels, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     ray_params \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mparameter\u001b[38;5;241m.\u001b[39mRayParams(\n\u001b[0;32m   1535\u001b[0m         node_ip_address\u001b[38;5;241m=\u001b[39m_node_ip_address,\n\u001b[0;32m   1536\u001b[0m         object_ref_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m         node_name\u001b[38;5;241m=\u001b[39m_node_name,\n\u001b[0;32m   1562\u001b[0m     )\n\u001b[0;32m   1563\u001b[0m     \u001b[38;5;66;03m# Start the Ray processes. We set shutdown_at_exit=False because we\u001b[39;00m\n\u001b[0;32m   1564\u001b[0m     \u001b[38;5;66;03m# shutdown the node in the ray.shutdown call that happens in the atexit\u001b[39;00m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;66;03m# handler. We still spawn a reaper process in case the atexit handler\u001b[39;00m\n\u001b[0;32m   1566\u001b[0m     \u001b[38;5;66;03m# isn't called.\u001b[39;00m\n\u001b[1;32m-> 1567\u001b[0m     _global_node \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mNode(\n\u001b[0;32m   1568\u001b[0m         head\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1569\u001b[0m         shutdown_at_exit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1570\u001b[0m         spawn_reaper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1571\u001b[0m         ray_params\u001b[38;5;241m=\u001b[39mray_params,\n\u001b[0;32m   1572\u001b[0m     )\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1574\u001b[0m     \u001b[38;5;66;03m# In this case, we are connecting to an existing cluster.\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_cpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_gpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\ray\\_private\\node.py:310\u001b[0m, in \u001b[0;36mNode.__init__\u001b[1;34m(self, ray_params, head, shutdown_at_exit, spawn_reaper, connect_only, default_worker)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_head_processes()\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m connect_only:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_ray_processes()\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;66;03m# we should update the address info after the node has been started\u001b[39;00m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\ray\\_private\\node.py:1357\u001b[0m, in \u001b[0;36mNode.start_ray_processes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdestroy_external_storage()\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;66;03m# Make sure we don't call `determine_plasma_store_config` multiple\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;66;03m# times to avoid printing multiple warnings.\u001b[39;00m\n\u001b[1;32m-> 1357\u001b[0m resource_spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_resource_spec()\n\u001b[0;32m   1358\u001b[0m (\n\u001b[0;32m   1359\u001b[0m     plasma_directory,\n\u001b[0;32m   1360\u001b[0m     object_store_memory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     huge_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ray_params\u001b[38;5;241m.\u001b[39mhuge_pages,\n\u001b[0;32m   1365\u001b[0m )\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_raylet(plasma_directory, object_store_memory)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\ray\\_private\\node.py:549\u001b[0m, in \u001b[0;36mNode.get_resource_spec\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    532\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoscaler overriding resources: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_resources\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    533\u001b[0m     (\n\u001b[0;32m    534\u001b[0m         num_cpus,\n\u001b[0;32m    535\u001b[0m         num_gpus,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         resources,\n\u001b[0;32m    539\u001b[0m     ) \u001b[38;5;241m=\u001b[39m merge_resources(env_resources, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ray_params\u001b[38;5;241m.\u001b[39mresources)\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_spec \u001b[38;5;241m=\u001b[39m ResourceSpec(\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ray_params\u001b[38;5;241m.\u001b[39mnum_cpus \u001b[38;5;28;01mif\u001b[39;00m num_cpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_cpus,\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ray_params\u001b[38;5;241m.\u001b[39mnum_gpus \u001b[38;5;28;01mif\u001b[39;00m num_gpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_gpus,\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ray_params\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;28;01mif\u001b[39;00m memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m memory,\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ray_params\u001b[38;5;241m.\u001b[39mobject_store_memory\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m object_store_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m object_store_memory,\n\u001b[0;32m    547\u001b[0m         resources,\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ray_params\u001b[38;5;241m.\u001b[39mredis_max_memory,\n\u001b[1;32m--> 549\u001b[0m     )\u001b[38;5;241m.\u001b[39mresolve(is_head\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead, node_ip_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_ip_address)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_spec\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\ray\\_private\\resource_spec.py:200\u001b[0m, in \u001b[0;36mResourceSpec.resolve\u001b[1;34m(self, is_head, node_ip_address)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to start raylet with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_accelerators\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccelerator_resource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccelerator_manager\u001b[38;5;241m.\u001b[39mget_visible_accelerator_ids_env_var()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvisible_accelerator_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_accelerators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# Try to automatically detect the number of accelerators.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     num_accelerators \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 200\u001b[0m         accelerator_manager\u001b[38;5;241m.\u001b[39mget_current_node_num_accelerators()\n\u001b[0;32m    201\u001b[0m     )\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m# Don't use more accelerators than allowed by visible accelerator ids.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m visible_accelerator_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\ray\\_private\\accelerators\\nvidia_gpu.py:67\u001b[0m, in \u001b[0;36mNvidiaGPUAcceleratorManager.get_current_node_num_accelerators\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m     props \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdapterCompatibility\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m     cmdargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWMIC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWin32_VideoController\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, props]\n\u001b[1;32m---> 67\u001b[0m     lines \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mcheck_output(cmdargs)\u001b[38;5;241m.\u001b[39msplitlines()[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     68\u001b[0m     num_gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([x\u001b[38;5;241m.\u001b[39mrstrip() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNVIDIA\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m num_gpus\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\subprocess.py:466\u001b[0m, in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    463\u001b[0m         empty \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    464\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m empty\n\u001b[1;32m--> 466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run(\u001b[38;5;241m*\u001b[39mpopenargs, stdout\u001b[38;5;241m=\u001b[39mPIPE, timeout\u001b[38;5;241m=\u001b[39mtimeout, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    467\u001b[0m            \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[1;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    572\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['WMIC', 'PATH', 'Win32_VideoController', 'GET', 'AdapterCompatibility']' returned non-zero exit status 2147749890."
     ]
    }
   ],
   "source": [
    "# Define hyperparameter search space\n",
    "config = {\n",
    "    \"DIM_EMBEDDING\": tune.choice([50, 100, 200]),\n",
    "    \"LSTM_HIDDEN\": tune.choice([25, 50, 100]),\n",
    "    \"LEARNING_RATE\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"EPOCHS\": tune.choice([3, 5, 7]),\n",
    "    \"BATCH_SIZE\": tune.choice([16, 32, 64])\n",
    "}\n",
    "\n",
    "#ray_constants.FUNCTION_SIZE_ERROR_THRESHOLD = 967619810\n",
    "\n",
    "def train_model(config, trainData, trainLabels):\n",
    "    DIM_EMBEDDING = config[\"DIM_EMBEDDING\"]\n",
    "    LSTM_HIDDEN = config[\"LSTM_HIDDEN\"]\n",
    "    LEARNING_RATE = config[\"LEARNING_RATE\"]\n",
    "    EPOCHS = config[\"EPOCHS\"]\n",
    "    BATCH_SIZE = config[\"BATCH_SIZE\"]\n",
    "    CONSTRAINTS = [(1, 4), (1, 5), (1, 6), (1, 10), (2, 0), (4, 2), (4, 4), (4, 5), (4, 6), (4, 7), (4, 10), \n",
    "                (5, 2), (5, 4), (5, 5), (5, 6), (5, 8), (5, 10), (6, 2), (6, 4), (6, 5), (6, 6), (6, 9), (6, 10),\n",
    "                (7, 2), (7, 4), (7, 5), (7, 6), (7, 7), (7, 10), (8, 2), (8, 4), (8, 5), (8, 6), (8, 8), (8, 10),\n",
    "                (9, 2), (9, 4), (9, 5), (9, 6), (9, 9), (9, 10), (10, 2), (10, 4), (10, 5), (10, 6), (10, 10)]\n",
    "    \n",
    "    numBatches = trainData.shape[0] // BATCH_SIZE\n",
    "\n",
    "    trainDataBatches = trainData[:BATCH_SIZE*numBatches].view(numBatches, trainData.shape[1], BATCH_SIZE)\n",
    "    trainLabelsBatches = trainLabels[:BATCH_SIZE*numBatches].view(numBatches, trainLabels.shape[1], BATCH_SIZE)\n",
    "\n",
    "    # Define your model, optimizer, and data loading here\n",
    "    model = baselineModel(len(vocab), labels, DIM_EMBEDDING, LSTM_HIDDEN, CONSTRAINTS)\n",
    "    #model = DataParallel(model)  # Utilize DataParallel for multi-GPU training\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        for batch in zip(trainDataBatches, trainLabelsBatches): \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = model.module.forwardTrain(batch[0], batch[1])  # Access the module inside DataParallel\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        tune.report(loss=loss.item())\n",
    "\n",
    "# Initialize Ray Tune\n",
    "ray.init()\n",
    "\n",
    "# Define scheduler and reporter\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    "    metric=\"loss\",  # Specify the metric parameter\n",
    "    mode=\"min\")  # Specify the mode parameter\n",
    "reporter = CLIReporter(metric_columns=[\"loss\", \"training_iteration\"])\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "analysis = tune.run(\n",
    "    lambda config: train_model(config, trainData, trainLabels),\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    trial_dirname_creator=lambda trial: str(trial))\n",
    "\n",
    "# Get best hyperparameters and corresponding results\n",
    "best_config = analysis.get_best_config(metric=\"loss\")\n",
    "best_loss = analysis.get_best_trial(metric=\"loss\").last_result[\"loss\"]\n",
    "\n",
    "print(\"Best config:\", best_config)\n",
    "print(\"Best loss:\", best_loss)\n",
    "\n",
    "# Shut down Ray Tune\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a625b66f05a6716aa62bf8e78e9986d1cdda2644917a6be9f119c99e84bc87a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
